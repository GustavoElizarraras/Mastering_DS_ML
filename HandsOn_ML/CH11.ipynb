{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Training Deep Neural Networks\n",
    "\n",
    "### Vanishing or exploding gradients\n",
    "\n",
    "Gradients must have equal variance before and aftes flowing through a layer in the reverse direction, for it to happen, the network needs the same number of inputs and neurons. The connection weights of each layer must be initialized randomly.\n",
    "\n",
    "- Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "- Uniform distribution between $-r$ nad $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$ \n",
    "\n",
    "Where $fan_{avg} = \\frac{fan_{in}+fan_{out}}{2}$\n",
    "\n",
    "There are other initializations and when to use them:\n",
    "\n",
    "<center><img src=\"img/initialization.png\"></img></center>\n",
    "\n",
    "Keras uses Glorot by default, to change it we use _kernel initializer=\"\"_, one option could be _\"he_normal\"_ or _\"he_uniform\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "# He initialization based on fan_avg ranther than fan_in\n",
    "he_avg_init = keras.initializers.VarianceScaling(scaling=2., mode='fan_avg',\n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky ReLU layer\n",
    "model = keras.models.Sequential([\n",
    "    ...\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    ...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PReLU layer (for big training sets, learning alpha on the go)\n",
    "model = keras.models.Sequential([\n",
    "    ...\n",
    "    keras.layers.PReLU(alpha=0.2),\n",
    "    ...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU\n",
    "# for x < 0: a*(exp(z)-1);  for x >= 0: z\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Adding an operation in the model just before or after the activation function of each hidden layer. It zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling and the other for shifting. In simple words, it learns the optimal scale and mean of each of the layer's input.\n",
    "\n",
    "__Batch Normalization Algorithm__\n",
    "\n",
    "<center><img src=\"img/batchN.png\"></img></center>\n",
    "\n",
    "Where:\n",
    "- $\\mu_B$ - vector of input means, evaluated over the whole mini-batch $B$ (one mean per input)\n",
    "- $\\sigma_B$ - vector of input standard deviations\n",
    "- $m_B$ - # of instances per mini-batch\n",
    "- $\\hat{x}^{(i)}$ - vector of zero-centered and normalized inputs for instance $i$\n",
    "- $y$ - output scale parameter vector for the layer\n",
    "- $\\bigotimes$ - element-wise multiplication\n",
    "- $\\beta$ - output shift (offset) parameter vector for the layer. Each input is offset by its corresponding shift parameter.\n",
    "- $\\epsilon$ - avoid division by 0, 1e-5, smoothing term\n",
    "- $z^{(i)}$ - output of the BN operation. It is the rescaled and shifted version of the inputs.\n",
    "\n",
    "Most implementations estimate the input $\\mu$ and $\\sigma^2$ by using a moving average during training, this are used only after it, while $\\beta$ and $y$ are learned through regular backpropagation.\n",
    "\n",
    "BN also acts as a regularizer, no need for other. It solves vanishing gradients, the network become less sensitve to the weight initialization, it allows bigger learning rates or the use of saturating activation functions.\n",
    "\n",
    "It becomes more computational demanding, but by substituing the previous layer's weights and biases with the new ones, the BN layer can be removed. TFLite does this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 18:50:27.497303: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Keras example\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu$ and $\\sigma$ are not affected by backpropagation, they are the Non-trainable params of the summary. From the batch layers, we sum them, and divide by 2, they are  $\\mu$ and $\\sigma$, the others correspond to $y$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's prove it\n",
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the BN layers before the activation function (depends on task)\n",
    "# Keras example\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # remove the activation function in the Dernse layer, bias=0\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # add it after the BN layer\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hyperparameter to tweak is the _momentum_, it is used to update the exponential moving averages and is a value close to 1, like 0.9, 0.99, 0.999... (more zeros if it is a big datasets and smaller mini-batches)\n",
    "\n",
    "Also, _axis_ is another important hyperparameter, with it we stablish how is the layer going to be normalized. \n",
    "- 2D [batch_size, features]: _axis=-1_, the last axis is going to be normalized\n",
    "- 3D [batch_size, height, width]: _axis=1_, will normalize all pixels in a  given column. _axis=[1, 2]_ will normalize all pixels independently.\n",
    "\n",
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clip the values during training so they never exceed some threshold.\n",
    "optimizer = keras.optimizers.SGD(clip_value=1.0) # between -1 and 1\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "# Using clipnorm will use the l2 norm. For e.g. clipnorm=1, gradient_vector=[0.9, 100]\n",
    "# It will clip it to: [0.00899964, 0.9999595], preserves the orientation but eliminates \n",
    "# the first component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading complex model, if it is retrained, model A will be affected\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# To solve this, we need to clone it and copy its weights\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "# dropping the last one\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "# new output layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing all layers except last\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "# Always compile after freezing it\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can unfreeze the reused layers and continue training to fine tune \n",
    "# the reused layers for task B\n",
    "history = model.fit(X_train, y_train, epochs=4,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# reduce the lr to acoid damaging the reused weights\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=16,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "First, restricted Boltzmann machines, now, GAN's and Autoencoders. Models are trained on unlabeled data, using unsupervised learning, then, it is fine-tuned for the final task on the labeled data with supervised learning.\n",
    "\n",
    "### Pretrainig on an auxiliary task\n",
    "If we don't have much labeled training data, using similar data to train a first neural net, then reuse the first layers for the real task.\n",
    "\n",
    "### Faster optimizers\n",
    "\n",
    "__Momentum Optimization:__ It cares about the previous gradients, it substracts the local gradient from the _momentum vector_ $\\eta m$ and updates the weights by summing $m$. $\\beta$ is a regularization parameter used to prevent the momentum from growing too large (usually 0.9).\n",
    "1. $m \\leftarrow \\beta m - \\eta\\Delta_\\theta J(\\theta) $\n",
    "2. $\\theta \\leftarrow \\theta + m $\n",
    "It get out local optima and reach the global optima faster.\n",
    "\n",
    "__Nesterov Accelerated Gradient:__ It measures the gradient of the cost function not al the local position $\\theta$, but slightly ahead in the direction of the momentum $\\theta + \\beta m$ (it is assumed it always goes to the optima, so looking ahead is good)\n",
    "\n",
    "1. $m \\leftarrow \\beta m - \\eta\\Delta_\\theta J(\\theta +\\beta m) $\n",
    "2. $\\theta \\leftarrow \\theta + m $\n",
    "<center><img src=\"img/momentum.png\"></img></center>\n",
    "\n",
    "__AdaGrad:__ It corrects the direction pinpointing to the global optima by going to the steepest dimensions.\n",
    "1. $s \\leftarrow s + \\Delta_\\theta J(\\theta) \\bigotimes \\Delta_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta\\Delta_\\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon} $\n",
    "\n",
    "The first step acummulates the square of the gradients into vector $s$, each $s_i$ accumulates the square of the partial derivative of the cost function with regard to parameter $\\theta$. If the cost function is steep along the $i^{th}$ dimension, then $s_i$ will get larger at each iteration.\n",
    "\n",
    "The second step is like Gradient Descent, but scale by $\\sqrt{s + \\epsilon}$ ($\\oslash$ element-wise division), $\\epsilon$ is to avoid division by 0, usually 10e-10\n",
    "<center><img src=\"img/gradientd.png\"></img></center>\n",
    "\n",
    "It usually stops too early, so it isn't recommended to train DNN.\n",
    "\n",
    "__RMSProp:__ Fixed AdaGrad, the decay rate $\\beta$ is usually 0.9,\n",
    "1. $s \\leftarrow \\beta s + (1-\\beta)\\Delta_\\theta J(\\theta) \\bigotimes \\Delta_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta\\Delta_\\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon} $\n",
    "\n",
    "__Adam:__ Adaptive moment estimation, it keeps track of an exponentially decaying average of past squared gradients:\n",
    "1. $m \\leftarrow \\beta_1 m - (1-\\beta_1)\\Delta_\\theta J(\\theta)$\n",
    "2. $s\\leftarrow \\beta_2 s - (1-\\beta_2)\\Delta_\\theta J(\\theta) \\bigotimes \\Delta_\\theta J(\\theta)$\n",
    "3. $\\hat{m} \\leftarrow \\frac{m}{1 - \\beta_1^t}$\n",
    "4. $\\hat{s} \\leftarrow \\frac{s}{1 - \\beta_2^t}$\n",
    "5. $\\theta \\leftarrow \\theta - \\eta\\hat{m} \\oslash \\sqrt{\\hat{s} + \\epsilon} $\n",
    "\n",
    "$t$ is the iteration number, step 1 computes an exponentially decaying average ($1-\\beta_1$) rather than an exponentially decaying sum. Step 3 and 4, $m$ and $s$ are initialized at 0, they will be biased toward 0 at the beginning of training, but it will boost them.\n",
    "\n",
    "The momentum decay hyperparameter $\\beta_1=0.9$, while the scaling decay hyperparameter $\\beta_2 = 0.999$, $\\epsilon=1e-7$, setting $\\eta=0.001$ is ok.\n",
    "\n",
    "__AdaMax:__ Sometimes more stable than Adam, it is a good try.\n",
    "1. $m \\leftarrow \\beta_1 m - (1-\\beta_1)\\Delta_\\theta J(\\theta)$\n",
    "2. $s\\leftarrow max(\\beta_2 s, \\Delta_\\theta J(\\theta)) $\n",
    "3. $\\theta \\leftarrow \\theta - \\eta\\hat{m} \\oslash \\sqrt{\\hat{s} + \\epsilon} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "# Nesterov\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "# RMSprop, ro is beta from the equations\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, rho=0.9)\n",
    "# Adam\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "# Other optimizers have its own class, check the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling\n",
    "There are many options to try, start high and decrease every number of epochs by a magnitude, start linear and then decay, etc.\n",
    "__Power scheduling:__ $\\eta (t)=\\frac{\\eta_0}{(1+t/s)^c}$\n",
    "\n",
    "__Exponential scheduling:__ $\\eta(t) = \\eta_0 0.1^{t/s}$\n",
    "\n",
    "__Piecewise constant scheduling:__ Use a learning rate depending on the epch\n",
    "\n",
    "__Performance scheduling:__ Reduce lr when error stops dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power scheduling, Keras assumes c=1\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential scheduling\n",
    "def exponential_decay(epoch):\n",
    "    # n0 and s hardcoded\n",
    "    return 0.01 * 0.1**(epoch/20)\n",
    "\n",
    "def exponential_decay2(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "exponential_decay_fn = exponential_decay2(lr0=0.01, s=20)\n",
    "\n",
    "lr_scheduler = kera.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history.model.fit(X, , y, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay3(epoch, lr):\n",
    "    #  it starts at the beginning of epoch 0\n",
    "    # it relies on the optimizer's initial lr, tune it\n",
    "    return lr * 0.1**(1/20)\n",
    "# When saving a model, the optimizer and it lr are saved, if we want\n",
    "# to use the model, the fit method start in epoch 0, so this could\n",
    "# damage the weights in this case. SOLUTION: use initial_epoch argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "# Same process as two cells upward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance scheduling\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train) // 32\n",
    "# use one schedule available in keras\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "# pass it to any optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "# This is specific to tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Overfitting though Regularization\n",
    "### $l_1$ and $l_2$ regularization\n",
    "$l_2$ regularization to constrain the weight, $l_1$ if we want a sparce model (many weights equal to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 regularization\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=keras.regularizars.l2(0.01))\n",
    "# the l2() function is called at each step during training. l1 works\n",
    "# the same way, for both of them: keras.regularizers.l1_l2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the times we apply the same regularizer, activation function, \n",
    "# initialization strategy to all hidden layers, in code it looks bad\n",
    "from functools import partial\n",
    "# thin wrapper for any callable, some default arguments\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                            activation=\"elu\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularized=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "A powerful regularization technique that boost 1-2% accuracy most of the times (that's a lot!). The neurons have a probability $p$ of being disabled in a training step, in regular DNN is set to 10-50%, CNN 40-50%, RNN 20-30%.\n",
    "\n",
    "After training we need to multiply each neuron's input connection weights by the keep probability $(1-p)$ or divide each neuron output by the same magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "# Dropout is only active during training, the training and validation loss may be \n",
    "# misleading, make sure to evaluate the training loss without dropout (after training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model  is overfitting, increase dropout rate, if underfitting, decrease it. Another option is to increase the dropout rate in large layers and decrease it in small ones, or maybe only use dropout after the last hidden layer.\n",
    "\n",
    "To regularize a self-normalizing network based on SELU, we should use _alpha dropout_\n",
    "\n",
    "### Monte Carlo (MC) Dropout\n",
    "Make predictions with dropout activated, then apply the mean of all of them, this gives a better measure of the model's uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in (range(100))])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model contains other layers that behave in a special way dring training (like BatchNormalization layers), we should replace the Dropout layers with the following MCDropout class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subclass the Dropout layer and override the call() method to force its training argument to True. We can define an MCAlphaDropout class by subclassing AlphaDropout instead. If we are creating a model from scratch, MCDropout is good, if we are using a pretrained model with Dropout, we need to create a new identical model using MCDropout and copy the model weights to our model.\n",
    "\n",
    "### Max-Norm Regularization\n",
    "For each neuron, it constrains the weights __w__ of the incoming connections such that $||w||_2 \\leq r$, where $r$ is the max-norm hyperparameter and $|| w ||_2$ is the $l_2$ norm. It does not add a regularization loss term to the overall loss function, it just rescale the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model fit() method will call the object returned by max_norm(),\n",
    "# passing it the layers weights and getting rescaled weights in return,\n",
    "# which then replace the layer's weights.\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                   kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and practical guidelines\n",
    "\n",
    "<center><img src=\"img/dnn.png\"></img></center>\n",
    "<center><img src=\"img/dnn2.png\"></img></center>\n",
    "\n",
    "- Normalize the input features\n",
    "- Use transfer learning\n",
    "- Unsupervised pretraining if we have a lot of unlabeled data\n",
    "- Pretraining on an auxiliary task\n",
    "\n",
    "Exceptions:\n",
    "- If we need a sparse model, $l_1$ regularization, or use Tensorflow Model Optimization Toolkit.\n",
    "- Low-latency model (fast): fewer layers, fold the Batch NOrmalization layers into the previous ones, use a fast activation function, reduce float precision.\n",
    "- Risk-sensitive application, use MC Dropout to get more reliable estimates."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
