{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Training Deep Neural Networks\n",
    "\n",
    "### Vanishing or exploding gradients\n",
    "\n",
    "Gradients must have equal variance before and aftes flowing through a layer in the reverse direction, for it to happen, the network needs the same number of inputs and neurons. The connection weights of each layer must be initialized randomly.\n",
    "\n",
    "- Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "- Uniform distribution between $-r$ nad $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$ \n",
    "\n",
    "Where $fan_{avg} = \\frac{fan_{in}+fan_{out}}{2}$\n",
    "\n",
    "There are other initializations and when to use them:\n",
    "\n",
    "<center><img src=\"img/initialization.png\"></img></center>\n",
    "\n",
    "Keras uses Glorot by default, to change it we use _kernel initializer=\"\"_, one option could be _\"he_normal\"_ or _\"he_uniform\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "# He initialization based on fan_avg ranther than fan_in\n",
    "he_avg_init = keras.initializers.VarianceScaling(scaling=2., mode='fan_avg',\n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky ReLU layer\n",
    "model = keras.models.Sequential([\n",
    "    ...\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    ...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PReLU layer (for big training sets, learning alpha on the go)\n",
    "model = keras.models.Sequential([\n",
    "    ...\n",
    "    keras.layers.PReLU(alpha=0.2),\n",
    "    ...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU\n",
    "# for x < 0: a*(exp(z)-1);  for x >= 0: z\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Adding an operation in the model just before or after the activation function of each hidden layer. It zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling and the other for shifting. In simple words, it learns the optimal scale and mean of each of the layer's input.\n",
    "\n",
    "__Batch Normalization Algorithm__\n",
    "\n",
    "<center><img src=\"img/batchN.png\"></img></center>\n",
    "\n",
    "Where:\n",
    "- $\\mu_B$ - vector of input means, evaluated over the whole mini-batch $B$ (one mean per input)\n",
    "- $\\sigma_B$ - vector of input standard deviations\n",
    "- $m_B$ - # of instances per mini-batch\n",
    "- $\\hat{x}^{(i)}$ - vector of zero-centered and normalized inputs for instance $i$\n",
    "- $y$ - output scale parameter vector for the layer\n",
    "- $\\bigotimes$ - element-wise multiplication\n",
    "- $\\beta$ - output shift (offset) parameter vector for the layer. Each input is offset by its corresponding shift parameter.\n",
    "- $\\epsilon$ - avoid division by 0, 1e-5, smoothing term\n",
    "- $z^{(i)}$ - output of the BN operation. It is the rescaled and shifted version of the inputs.\n",
    "\n",
    "Most implementations estimate the input $\\mu$ and $\\sigma^2$ by using a moving average during training, this are used only after it, while $\\beta$ and $y$ are learned through regular backpropagation.\n",
    "\n",
    "BN also acts as a regularizer, no need for other. It solves vanishing gradients, the network become less sensitve to the weight initialization, it allows bigger learning rates or the use of saturating activation functions.\n",
    "\n",
    "It becomes more computational demanding, but by substituing the previous layer's weights and biases with the new ones, the BN layer can be removed. TFLite does this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 18:50:27.497303: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Keras example\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu$ and $\\sigma$ are not affected by backpropagation, they are the Non-trainable params of the summary. From the batch layers, we sum them, and divide by 2, they are  $\\mu$ and $\\sigma$, the others correspond to $y$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's prove it\n",
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the BN layers before the activation function (depends on task)\n",
    "# Keras example\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # remove the activation function in the Dernse layer, bias=0\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # add it after the BN layer\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hyperparameter to tweak is the _momentum_, it is used to update the exponential moving averages and is a value close to 1, like 0.9, 0.99, 0.999... (more zeros if it is a big datasets and smaller mini-batches)\n",
    "\n",
    "Also, _axis_ is another important hyperparameter, with it we stablish how is the layer going to be normalized. \n",
    "- 2D [batch_size, features]: _axis=-1_, the last axis is going to be normalized\n",
    "- 3D [batch_size, height, width]: _axis=1_, will normalize all pixels in a  given column. _axis=[1, 2]_ will normalize all pixels independently.\n",
    "\n",
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clip the values during training so they never exceed some threshold.\n",
    "optimizer = keras.optimizers.SGD(clip_value=1.0) # between -1 and 1\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "# Using clipnorm will use the l2 norm. For e.g. clipnorm=1, gradient_vector=[0.9, 100]\n",
    "# It will clip it to: [0.00899964, 0.9999595], preserves the orientation but eliminates \n",
    "# the first component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading complex model, if it is retrained, model A will be affected\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# To solve this, we need to clone it and copy its weights\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "# dropping the last one\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "# new output layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing all layers except last\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "# Always compile after freezing it\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can unfreeze the reused layers and continue training to fine tune \n",
    "# the reused layers for task B\n",
    "history = model.fit(X_train, y_train, epochs=4,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# reduce the lr to acoid damaging the reused weights\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=16,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
