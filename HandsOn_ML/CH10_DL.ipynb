{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Introducing DL and TF\n",
    "\n",
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading fashion MNIST dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# datasets insights\n",
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation set and scaling images\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index assigment to the class images\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 15:18:33.672943: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Model using the Sequential API\n",
    "model = keras.models.Sequential()\n",
    "# flatten computes the same as using X.reshape(-1, 1)\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "# softmax for multi class classification\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass a list of layers\n",
    "model = keras.models.Sequential([\n",
    "# flatten computes the same as using X.reshape(-1, 1)\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dense layer are a lot of trainable parameters, but this can mean overfitting, especially if we don't have a lot of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04094823 -0.06484979 -0.06214181 ...  0.02847133 -0.03779142\n",
      "  -0.07006058]\n",
      " [-0.01780953  0.03537125  0.02749476 ... -0.03493334  0.03907974\n",
      "   0.05217421]\n",
      " [-0.01290033 -0.0607337   0.04049524 ... -0.01717551  0.03696581\n",
      "  -0.06738206]\n",
      " ...\n",
      " [ 0.05619214  0.02518968  0.00406668 ... -0.04470732  0.00969991\n",
      "  -0.02506723]\n",
      " [-0.00665344 -0.04226232  0.01060253 ... -0.05347344 -0.00094701\n",
      "   0.06651762]\n",
      " [-0.06754795 -0.03428278 -0.0381333  ... -0.02397911 -0.01714364\n",
      "   0.03286845]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Accesing parameters\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ever want another weights or biases initialization, we can\n",
    "set _kernel_initializer_ or _bias_initializer_ when creating a layer.\n",
    "\n",
    "If there isn't the input_shape, Keras will figure it out when feeding the data or calling the _build()_ method. But the random initialization won't happen until then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model and specifying the loss function and the optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# Is the same using:\n",
    "# keras.metrics.sparse_categorical_accuracy and keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use __sparse_categorical_crossentropy__ loss because we have sparse labels (10 classes, we only predict one), we would use __categorical_crossentropy__ if we had one target probability per class for each instance (ex, one hot vectors, class1=[1, 0 ... 0]). If it were binary classification, in the last layer we would use the __sigmoid__ activation function and __binary_crossentropy__ as the loss.\n",
    "\n",
    "To convert to 1-hot-vector from sparse, use: _keras.utils.to_categorical()_ . The other way around: _argmax(axis=1)_ \n",
    "\n",
    "Using _optimizer=\"sgd\"_ defaults to lr=0.01. Better: _keras-optimizers.SGD(lr=???)_\n",
    "\n",
    "Since this is a classifier, an useful measure is _accuracy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 15:18:38.091524: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-02 15:18:38.137957: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2901210000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 6s 2ms/step - loss: 0.7127 - accuracy: 0.7648 - val_loss: 0.5502 - val_accuracy: 0.8094\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4891 - accuracy: 0.8319 - val_loss: 0.4497 - val_accuracy: 0.8460\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4444 - accuracy: 0.8446 - val_loss: 0.4365 - val_accuracy: 0.8532\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4160 - accuracy: 0.8540 - val_loss: 0.4153 - val_accuracy: 0.8544\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3965 - accuracy: 0.8613 - val_loss: 0.3848 - val_accuracy: 0.8638\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3801 - accuracy: 0.8655 - val_loss: 0.3746 - val_accuracy: 0.8702\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3654 - accuracy: 0.8716 - val_loss: 0.3727 - val_accuracy: 0.8698\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3538 - accuracy: 0.8747 - val_loss: 0.3559 - val_accuracy: 0.8742\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3432 - accuracy: 0.8785 - val_loss: 0.3583 - val_accuracy: 0.8720\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3348 - accuracy: 0.8805 - val_loss: 0.3540 - val_accuracy: 0.8752\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3251 - accuracy: 0.8840 - val_loss: 0.3822 - val_accuracy: 0.8606\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3175 - accuracy: 0.8869 - val_loss: 0.3709 - val_accuracy: 0.8682\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3107 - accuracy: 0.8886 - val_loss: 0.3432 - val_accuracy: 0.8768\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3038 - accuracy: 0.8913 - val_loss: 0.3247 - val_accuracy: 0.8814\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2974 - accuracy: 0.8943 - val_loss: 0.3216 - val_accuracy: 0.8864\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2900 - accuracy: 0.8957 - val_loss: 0.3239 - val_accuracy: 0.8818\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2850 - accuracy: 0.8970 - val_loss: 0.3204 - val_accuracy: 0.8868\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2787 - accuracy: 0.8999 - val_loss: 0.3320 - val_accuracy: 0.8766\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2746 - accuracy: 0.9009 - val_loss: 0.3210 - val_accuracy: 0.8814\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2685 - accuracy: 0.9035 - val_loss: 0.3036 - val_accuracy: 0.8904\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2638 - accuracy: 0.9045 - val_loss: 0.3029 - val_accuracy: 0.8918\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2594 - accuracy: 0.9074 - val_loss: 0.3182 - val_accuracy: 0.8844\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2550 - accuracy: 0.9087 - val_loss: 0.3086 - val_accuracy: 0.8900\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2516 - accuracy: 0.9097 - val_loss: 0.3096 - val_accuracy: 0.8888\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2461 - accuracy: 0.9112 - val_loss: 0.3210 - val_accuracy: 0.8854\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2429 - accuracy: 0.9131 - val_loss: 0.3123 - val_accuracy: 0.8854\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2382 - accuracy: 0.9142 - val_loss: 0.3084 - val_accuracy: 0.8872\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2342 - accuracy: 0.9152 - val_loss: 0.2989 - val_accuracy: 0.8934\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2304 - accuracy: 0.9178 - val_loss: 0.3036 - val_accuracy: 0.8922\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2271 - accuracy: 0.9174 - val_loss: 0.3024 - val_accuracy: 0.8892\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "# Keras displays the metrics during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing a validation_data argument, we can set _validation_split_ =0.2 (for example), to get a 20% validation split from the training.\n",
    "\n",
    "If the training set was very skewed, with some classes being overrepresented and other underrepresented, setting _class_weight_ argument in the __fit()__ method, would give larger weight to the underrepresented classes and viceversa, this weights are used when computing the loss. If we need per-instance weights (some data is better than other), _sample_weight_ argument can help us, if both of these are used, Keras multiply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFBCAYAAABEo8fdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABPbklEQVR4nO3deXxU1f3/8deZPckkk8m+EBL2NQlLQIWiqMWt1q0qtdUfatW2am3r99vWWtuvX2s3rbb2W6uite5V1GLdsVYiWlEEZN8JBBK27Pvs5/fHnQxJmECAwGT5PB+Pedw79965c+Yw5D3nLucorTVCCCGEiB1TrAsghBBCDHYSxkIIIUSMSRgLIYQQMSZhLIQQQsSYhLEQQggRYxLGQgghRIwdMYyVUk8qpQ4opdZ1s14ppf6klNqmlFqjlJrS+8UUQgghBq6etIyfAs47zPrzgVHhx03AI8dfLCGEEGLwOGIYa62XALWH2eRi4Blt+BRIVkpl91YBhRBCiIGuN84Z5wK7OzyvCC8TQgghRA9YTuabKaVuwjiUTVxc3NS8vLxe23coFMJkkuvRupJ6iU7qJTqpl+ikXqKTeomuu3rZsmVLtdY6PdpreiOMK4GOqTokvOwQWuv5wHyAkpISvXz58l54e0NpaSmzZ8/utf0NFFIv0Um9RCf1Ep3US3RSL9F1Vy9KqfLuXtMbP2leB/5f+KrqU4EGrfXeXtivEEIIMSgcsWWslPo7MBtIU0pVAP8DWAG01o8CbwMXANuAVuC6E1VYIYQQYiA6Yhhrra86wnoN3NJrJRJCCCEGGTnzLoQQQsSYhLEQQggRYxLGQgghRIxJGAshhBAxJmEshBBCxJiEsRBCCBFjEsZCCCFEjEkYCyGEEDEmYSyEEELEmISxEEIIEWMSxkIIIUSMSRgLIYQQMSZhLIQQQsSYhLEQQggRYxLGQgghRIxJGAshhBAxZol1AYQQQojjEvCCtxl8TeBtCs83G/O+ZmO9UqBMoMzhaZeHKcoysx1GffmkfAQJYyGEED0TCoK/DQIe8LcS11oB+9dD0A+hQHjqj/I80GF51+ddXne4fQQ8B0M2Er7NxvoTweGCO3admH13IWEshBB9XSgYDipfl4DydQ62oB+CXiO0Aj5jGvQdfN7tOq8Rsv428Ld2CtyDy9uM13dwCsCyXvqMZhuYrGC2hKfhh6nD1GIHuxMS0sGeCDan8dzm7Pzcngi2xIPrLA5AG/WoQz1/oHrpwx2ZhLEQQhwNrY1g8rYfEm3sMg3PexrB1xIOzHBItgdqoMN8p+W+8DJf5xYiupc/hDKCzWw3phY7WOPB6jCmjiSwZII1LvzosK79ucXBhq1ljJ9Y1CEwLV0CtONzS+dg7Ri2JrNxGHkQkzAWQvRvWhuh52vpfJ6w/bxhx3l/61EcMg0vDwUoqa+BVaGDYRsKHLlcFgfYEozAM4dbdWZbOITCy6wuY5nFFl5n67CN7WCYdZyP1mqMzIf3ZXGE5x3hfds7z5utvRJ+BxpLGT9h9nHvR0gYCyFOhFAI/C1G67C9lehtAm9Dh/n2lmMPzxd2XB7wdg7ZnrYc24PosC02i7G+/bCoyYnHYcaZO9w4/OlIMqb2RLAnhR+JHdYlhQ+N2k5oFYuBRcJYiIEo6A+fE/SHD4l6O5wz9B08PBpZ5oWgj+w96+CzLeFDp10Pm/oi23U+1OoFv6fz4VlvI0cOSBVuOUY5rBmtJWixH3xusXU4T5jQ4dxh+Hn7fPs5w/ZtzMf2J29daSmzZ88+ptcK0RMSxkL0JcFAl0OtXQKufept6rCs4dBlQd8xvf0YgC1dFpo6BKHZHv2wqsUOKcMOthIdSV3mXR1alEnGvDXBuJ3kJNNao/1+tM938NH1uc9HKDLvx75pI97cXKx5eZgcjpNe5sFEh0IEqqsJ7NmDP/LYi3/PHoINDSiLBWW1Gg+bNTKP9eB854cNk8OOKcGJKdGJOTERkzMRc6ITU2Ki8bDF/iiGhLEQRyMUgkCb0RIMhB+RK0/bjNZiZH1b56tUo90D2fH8prfJeM0RqYOB1j51ZkLqqA4h6Dx4cU57WHacRubtHc4p2vhk2QpmfGl257DthxfW6GAQ/+7deLZswbtlK94tW/Bu3Yq/shLtO/ofKslA2SOPglJYsrKw5ecbj6FDsRUY89a8PEx2e4/2F/L5CBw4QGD/fvz79hHYtx///oPTUGNT59AJB8/hAkdZrUboJCZhTkrC7ErClJSE2eXCnJSEKTEJU0I8Kkb/nlprtNeL9ngINjZ2CtmOj8DevWh/51uVTElJWHNyMCcnQyBAqKXF+AF1uIfPZ1xP0APKag0HsxOz0whoc6ITszuF7Hv+9wTUxqEkjMXg4feApx7a6klq2Ahb/V1alB2vju1mub/12N/fEtf5Ngx7IiRmd7gVo8PyyC0aXVuYSYS0lUBNLYGqAwSqqjo/qqsJVJWjW1oxp6RgTk3BkpqGJTUVc1oqltQkLGmpxvPUNEwJCZ3+OPvsOyEhNfI81NbWef8Hqg59z6oqdCCAfcxo4iZMwBF+2AoKUGbzsddXD2itCVRVGYG7NRy6W7bg3b4d7fEYGymFdWge9lGjcJ55JiaHHWWzGQFm6/iwomw2TLauy20sX7qUwrR0fOU78ZWX4ysvp2nRIoL19QcLoxSW7A5BnV+A2eUy6mj/Pvz79hPYtw///v0Ea2oO+Sym+HgsWVlYszKx5uRAINgpXEJtHnRjU/TgCQSMaVvb4QPIYsGcmGiEczikzUlGSB/LbTxJlRVUvv4GoXDIhrwetMeL9no7LDOm2uuNvhOlsKSnY83OJm7iBKznzMGSk4M1OxtrTi7W3BzMTudRlw2MH2Xa7zd+ADQ3E2pqItjURKi5hVBzeL6pOTwfXt9srPftLIeKymN632MhYSz6vkhrtMM9kL6Wg9NwwNJWd3A+2rIOrc4pAF90fSPV4cKc8OHUODe48w8+tzmNWzwscV2m4Yc1rsN8l+0Oc75SBwIE6+oI1NQQqK4hWFFNoLqGQE3ZIcEXamw8dAcmE5bUVOOPWnoGpoJ4ArV1+MvLaVux0giNKH+kld0eDuo0LCkpuJqbKP/rk5FgDzU3H/peFguWtDTjvYYMIW7yZDApvBs2UvfSgkgIqvh4HGPHhsN5PHETJmAbNgxl6fmfHR0KEWxoIFhdbfzQqK4mUFWNf8+eSPB2DERzWhqO0aNwz52LffRo7KNHYR8xAlN8fI/fM5rA3r24opwzDjY0RMLZt7Mc365d+MrLaXznXUINDQfL5XJhycrCkpWJY+JELFmZWDMzsWQa4WvJyjrmwOlIh0KEWloINjQSamwg2NhIsKGRYGMDoa7zjU1GC3X3bkKtx/Yj0+b30+ZKwmR3oBwOTHY7Jrfb+MFjd6Ac9vA6+8FtHHZMzkSsOdlYc3KwZGWdsMPEymw2fhA6HEarug+TMBYnVjBghGFrrRGMbXXQFp7vuMxTfzBsfa1dOh/oyaHbMJsTHMlGiMYlQ+oIY+pI7jB1s2bLLoqmf6nDFbHh1mgvn8NsP1zq27WLQFU1gZoagjXhoK2tIVhdYyyrq+s+LNPTsaSlYR8+nIRTTsGSkW4sCy+3pKdjTkk5bCtUBwIEamsJ1tYaYd9ehg7l8e/bh6W2Fp2Xh33sWBLSO7xPejqW9PB7JSejuqknHQjgLSvDs34DnvXr8WzYQP0rr6CfNf4NlcPRIaAnYM3NNX6EVFcTqDZ+AASrOgRvTQ0EDr2NyBQfj33UKBLnfBn7qNGR4LWkpBzjv9SxMbtcxBUVEVdUdMi6QF0doaYmLOnpmOLiTkp5lMlktHwTE4HcE/5+pXJhW6+RMBaHp7VxTtMTviXF04D2NBCq2U+gah/B6irjj3xdPYH6RkItbSi8mGjDpI2HsoQwWXTkoczheavC5HShEpPBkYw2xREkjZDJStBsJRQyEQwpQgFF0KcJeUMEPUGCbX5CrX6CbT6UyUx8yWScp59O3LQZKEfPWkC1NaWQN733qikcut5t2/Bu2x6ebsNXVnbIOUoVH2+0YlNTseYPJW7KlA6HkdM6HEZOxeR09so5PmWxYM3IwJqRcdjtSktLKTyOP67KYsExejSO0aPh0ksAo258O3bgWb+etnBANyxcSN3zz3d+sdls1EtaGua0VOxjxhg/NtLSjB8CaWmYU4353qqXE8nidoPbHetiiH5CwniA0YEA/n378VdW4q+sJP7zZdRs2xbpCF2Z1MEO04NelLcBvPXgqUd56sETbrm2NhBoaiPY4ifgUQS9JgIeszH1miAU/Q+hsoCONGTs4ceRtIHFH7UF1GnfVqtxnisxEVNSIub0dEJtbdS88A9qnn0Zk9NJwowZJMz6Es5Zs7BmZfW84npIB4P4KyqMsN1qBK53+3YjdDucE7PkZGMfOZKEGTOwjxiBbdgwo0WbknLch0z7G2U2Yx85EvvIkbguvhgIB3R5OYH9+zGnpGJJTztsi1uIgU7CuJ/Rfj/+ffvCYbsnErr+ykp8eyoJ7NtvnGMNSwQOHPO72VD2eCyJcZhdTqy5Lhwp7nDLJQNLRjbmjBwsGVnGH1R3MspmM66a9HgIeTzo1lZCbW3Go7WNUFsrOjLfhvYY8zoQCN92kGRMk5LCoXtw2t2VqsGmJlqWLqXlo49o/uhjmt57DwD76NHhYD6d+CmTUT08L6W1JlhXh2/nTuM84M6dBx/l5dFD99RTjcAZNRLb8BGYnQnHXOuDgTKbsQ8fjn348FgXRYg+QcK4jwo2t+Ddshnv5s14Nm3Cu2UL/soKAtU1EOpwblGBJcmGNdFMfFIQa2YQm60Za5wXa0IQS1w4mDVoRzIk5kBSLjox17iS15mNdmZDYhY4jPNtWmuUUpjd7mNqxSmlUHFxxnmyk3CYzpyYSNI555B0zjlorfFu3UrLRx/T/NFH1D7zLLV/fRJTfDzxp52Gc9YsnLO+BBh17CvvHLTt4dvpIimLBVteHraCAhJmzgy38kZgGzFSQlcI0SskjGNMh0L4KyvxbNyEd+1KvBvW4Nm6A/+Busg2JpvGnuQjwRnEOi6INSGANSFoPFzxqKRUiHdDQprxiD84XbN9L0WzLgDXEONWmQFOKRU5Z5n6resJNrfQuuwzmpcsoWXJRzT/+98ApCcksKWlpdNrLTnZ2AsKcF34FWwFBcbtKQUFWHNzj+oKYCGEOFryF6aXaa2h/Z6/KDehB/ftwLv6czwb1+Mt24W3spaQt/2wssaWGMSR7Ce5KIg9NxnHyGFYCkaj3AXgzID41IOBG59q3DZzGLX1pZAx9kR/7D7L7Ewg8ayzSDzrLLTW+HbsoOWjj9jx0cfkT58eCVxb/lDpWUkIETMSxkcQamnBF741xb9rF77yXcZtKvv2EfIb3ejh6xy6PWGyhrAnB3CNjsc+NAvHqBHYxxdhyh5rdCuYPNToflD0GqVU5Dzlmvx80uSWDCFEHyFhDAQbG8MhW24E7i4jfH27yglWVXfa1pySgm3oUBwTxhs3tVstKO1H+RtQ3lqUpwbVVoXyVKNUCEza6OknOQeVmocpYzj2iVOwji1BJQ895o7rhRBCDByDNgl0KETt356i5q9/JVhb22mdJSMD29ChOE8/HdvQfGz5Q7Hm5WEbOhSz8kD5f2D3Mti/FvatM24FMgMOYGgeZE6ErInGNHOi0dI1ndhuAYUQQvRfgzKM/fv3s+cnd9D66ackzJpFwqmnGoE7dCi2vLzOveU07Yfyj2HHE7D4P1C92VhucUDGOBj7FcgqDAfvBKOXJyGEEOIoDLowblz0Hnt/8Qu030/2r+7FddllnXvyadwDW9+EnR8ZLeCabcZymxOGngrFX4eCL0HOZDmnK4QQolcMmjAOtbSw7ze/oeGVV3EUFpJ7/33YCgqgoQJ2fmyE787/QN0O4wX2JMifAVPmQcFMyCqW87tCCCFOiEGRLm1r11L53/+Nf9duUr/9bdJvvcUYjPqzx+CdHxsbOZIhfyZMv9GYZhXKeV4hhBAnxYAOYx0MUvP441T9+WEs6enkP/M08dOmGSt3fQqL7oRR58LZv4CM8b0+Yo8QQgjREwM2jP2VlVT+5Ce0LV9B0gXnk3X33ZiTkoyVzVXw8rXgyoOvPQ4OV0zLKoQQYnAbkGHc8OZb7Pvf/4VQiJzf/Zakiy46eJFWKAivfssYQ/eG9yWIhRBCxNyACuNgUxP7fvlLGl9/g7jJk8m5/z5sQ4Z03qj0N7DjQ7j4YeO8sBBCCBFjAyaMrdu2s+OX9+Lft4+0791K2re/fWjn/lv/BUvuh8lXGw8hhBCiD+jRFUtKqfOUUpuVUtuUUndEWT9UKbVYKfWFUmqNUuqC3i9q9+oXvob7gQdAKfKfe5b0W245NIjrd8E/boTMQrjg9yezeEIIIcRhHTGMlVJm4GHgfGA8cJVSanyXze4CFmitJwNfB/7S2wU9nPhp02j70pcY9tpC4idPPnSDgBcWzDPOF1/5NFjjDt1GCCGEiJGetIynA9u01mVaax/wInBxl200EL5UGRewp/eKeGS2Ibk0ffMbmJ3djNe76E7YsxIueQRSR5zMogkhhBBHpLTWh99AqcuB87TWN4SfXwOcorW+tcM22cB7gBtIAL6stV4RZV83ATcBZGZmTn3xxRd763PQ3NyMM0oYZ+z/kPEbH2RX3iWUjbiu196vv+iuXgY7qZfopF6ik3qJTuoluu7q5cwzz1yhtS6J9preuoDrKuAprfUDSqnTgGeVUhO11qGOG2mt5wPzAUpKSvTsXhxPtrS0lEP2d2AT/OdRGHoaQ+c9wdBB2Jd01HoRUi/dkHqJTuolOqmX6I6lXnpymLoSyOvwfEh4WUffAhYAaK2XYgwmmHZUJelt3mZYcA3YEuDyv8mgDkIIIfqsnoTx58AopdQwpZQN4wKt17tssws4G0ApNQ4jjKt6s6BHRWt44zZjxKXLn4Sk7JgVRQghhDiSI4ax1joA3AosAjZiXDW9Xil1j1LqovBm/wXcqJRaDfwduFYf6WT0ifT5E7DuVTjrLhh2esyKIYQQQvREj84Za63fBt7usuwXHeY3ADN7t2jHqGI5vPtTYwCImT+MdWmEEEKIIxpYwxS11hr3Eydlw6WPyihMQggh+oUB0x0mOmT0sNVyAK5fBPEpsS6REEII0SMDJozzy1+Gne/DVx6E3CmxLo4QQgjRYwPjOO72xRTs/DsUzYWS62NdGiGEEOKoDIwwtsZT5y6GC/8A7eMWCyGEEP3EwAjjoaewpvh/jQ4+hBBCiH5mYISxEEII0Y9JGAshhBAxJmEshBBCxJiEsRBCCBFjEsZCCCFEjEkYCyGEEDEmYSyEEELEmISxEEIIEWMSxkIIIUSMSRgLIYQQMSZhLIQQQsTYgAjjT7ZV88cVHjz+YKyLIoQQQhy1ARHGjZ4Aq6qCrKtsiHVRhBBCiKM2IMJ4ar4bgOXldTEuiRBCCHH0BkQYpyfayYxXLN8pYSyEEKL/GRBhDDAy2czKXXVorWNdFCGEEOKoDJgwHuU2UdviY0d1S6yLIoQQQhyVARTGZkDOGwshhOh/BkwYZycoXHFWVsh5YyGEEP3MgAljk1JMzXezvLw21kURQgghjsqACWMwbnHaXtVCXYsv1kURQgghemxAhXFJ+H7jFXLeWAghRD8yoMK4OC8Zq1nJRVxCCCH6lQEVxg6rmQk5LlbIeWMhhBD9yIAKYzDOG6+uaMAbkEEjhBBC9A8DLoxL8t34AiHWVTbGuihCCCFEjwy4MJ5a0H4RlxyqFkII0T8MuDDOSHQwNCVeBo0QQgjRbwy4MAbjUPWKchk0QgghRP8wIMN4aoGbmhYfO2taY10UIYQQ4ogGZBiX5KcAsHynnDcWQgjR9w3IMB6V4STJYZGeuIQQQvQLAzKMTSbFlHy39MQlhBCiXxiQYQzGRVzbDjRT3yqDRgghhOjbBmwYTw2fN5ZD1UIIIfq6ARvGk/KSsZhk0AghhBB934AN4zibmQk5SdIyFkII0ecN2DAG41D16t31+AKhWBdFCCGE6NaADuOSAjfeQIj1expiXRQhhBCiWz0KY6XUeUqpzUqpbUqpO7rZ5kql1Aal1Hql1Au9W8xjU5LfPmiEHKoWQgjRdx0xjJVSZuBh4HxgPHCVUmp8l21GAT8FZmqtJwA/6P2iHr2MJAd5KXEyaIQQQog+rSct4+nANq11mdbaB7wIXNxlmxuBh7XWdQBa6wO9W8xjV5KfwnIZNEIIIUQf1pMwzgV2d3heEV7W0WhgtFLqP0qpT5VS5/VWAY/X1Hw31c1edtXKoBFCCCH6Jksv7mcUMBsYAixRShVqres7bqSUugm4CSAzM5PS0tJeentobm6Ouj/dZFxJ/dy7nzAz19pr79dfdFcvg53US3RSL9FJvUQn9RLdsdRLT8K4Esjr8HxIeFlHFcBnWms/sEMptQUjnD/vuJHWej4wH6CkpETPnj37qAp7OKWlpUTbXyikuW/FezTHZTF7dmGvvV9/0V29DHZSL9FJvUQn9RKd1Et0x1IvPTlM/TkwSik1TCllA74OvN5lm9cwWsUopdIwDluXHVVJThCTSTFlqJsV5TKcohBCiL7piGGstQ4AtwKLgI3AAq31eqXUPUqpi8KbLQJqlFIbgMXAj7TWNSeq0EerJN/Nlv3NNLT6Y10UIYQQ4hA9OmestX4beLvLsl90mNfA7eFHnzO1wLjfeOWuOs4cmxHj0gghhBCdDegeuNpNykvGbFIsl0PVQggh+qBBEcbxNgsTcpKk8w8hhBB90qAIYzDuN15dUY8/KINGCCGE6FsGVRh7/CHW72mMdVGEEEKITgZNGJfkpwCwfKecNxZCCNG3DJowznI5yE2OkxGchBBC9DmDJozBGN9YBo0QQgjR1wyIMN7RsIPX6l4jpA9/cVZJvpuqJi+7a9tOUsmEEEKIIxsQYbyueh3/bvw3/yr/12G3m9p+3ljuNxZCCNGHDIgwvmDYBWRZs/jLqr8QDAW73W5MViKJdoucNxZCCNGnDIgwNpvMXOC6gLKGMt7Z+c5htlNMGposYSyEEKJPGRBhDFAcX8xo92geWfUIgVCg2+1K8lPYvL+JhjYZNEIIIUTfMGDC2KRM3DLpFnY17eKN7W90u11JgRut4Ytd0joWQgjRNwyYMAY4M+9MJqRO4NHVj+IPRm/5tg8aIYeqhRBC9BUDKoyVUtw6+Vb2tOxh4baFUbdJsFsYl50og0YIIYToMwZUGAPMzJnJpPRJPLbmMbxBb9RtSvJTWLVbBo0QQgjRNwy4MG5vHR9oPcDLm1+Ous3UfDdt/iAb98qgEUIIIWJvwIUxwCnZpzAtaxpPrH2CtsChvW2VFLgB5FC1EEKIPmFAhjHArZNupcZTw4ubXjxkXbYrTgaNEEII0WcM2DCekjmFmTkzeXLdk7T4Ww5ZPzXfzfLyWhk0QgghRMwN2DAGuGXSLdR763luw3OHrCspcLO/0UtFnQwaIYQQIrYGdBgXphcye8hsnt7wNI2+zhdrTc03zhvLoWohhBCxNqDDGOCWybfQ5GvimfXPdFo+NisJp90iIzgJIYSIuQEfxmNTxjInfw7PbXyOOs/BVrDZpJg8NFmuqBZCCBFzAz6MAW4uvplWfyt/W/+3Tsun5rvZvL+JdZUNMSqZEEIIMUjCeKR7JOcPO58XN71IdVt1ZPnlU4eQ44rjyseW8sGm/TEsoRBCiMFsUIQxwHeLv4s36OWva/8aWTbEHc/Cm2cwPD2BG55ezrOflsewhEIIIQarQRPGBa4CLhpxEQs2L2B/y8FWcEaSg5duOo3ZYzL4+Wvr+PXbGwmF5N5jIYQQJ8+gCWOAbxd9m5AO8fjaxzstT7BbmH/NVK45NZ/5S8q49e8r8fiDMSqlEEKIwWZQhfGQxCFcOupSXt36Knua93RaZzGbuOfiCdz1lXG8s24f33j8U2qao4/6JIQQQvSmQRXGADcV3YRC8diaxw5Zp5TihlnD+cs3prB+TyOXPfIJZVXNMSilEEKIwWTQhXFWQhZXjL6Cf277J7sad0Xd5vzCbP5+06k0eQJc9sgnfL5TOgYRQghx4gy6MAa4ofAGrCYrj6x+pNttpgx1s/DmGaTE2/jm45/xxuo93W4rhBBCHI9BGcbp8el8fezXeavsLcrqy7rdLj81gVe/O4PiPBff+/sXPFK6XUZ5EkII0esGZRgDXDfxOhwWB39e9WdCOtTtdu4EG89+6xS+WpzD797dxJ0L1xEIdr+9EEIIcbQssS5ArKQ4Urh63NU8vvZxZr80m2lZ0zgl+xSmZ00nPykfpVRkW4fVzENzJzE0JY6HF29nT30bD39zCk77oK0+IYQQvWhQp8nNk26mwFXAZ3s/49O9n/Je+XsAZMRncErWKUzPns4pWaeQ7czGZFL86Nyx5Lnj+dlr67j8kU/41aWFkaEYhRBCiGM1qMPYYrJw0YiLuGjERWit2dW0i8/2fsayfcv4z57/8EbZGwDkJeYxPWs6p2SfwpcLp5GTPI3/fnk1X3vkEy4ozOLH546lIC0hxp9GCCFEfzWow7gjpRT5SfnkJ+Vz5ZgrCekQ2+q3sWzvMpbtW8Z7O9/j1a2vAjAyeSQXn30K3rqpvLK0in9t2M/Vp+Zz21mjcCfYYvxJhBBC9DcSxt0wKROj3aMZ7R7N1eOvJhgKsql2E5/t+4xle5fxj60v4ws9T/G0YiytM3l6qZ9XVlRw65kjmTejAIfVHOuPIIQQop+QMO4hs8nMhLQJTEibwPUTr6feU88/t/+TBZsXsMvzF7ILk4n3nsZv3z/AM0vL+fF5Y/hqUQ4mkzryzoUQQgxqg/bWpuOV7Ehm3oR5vHHpGzw25zFKMqdwwLSIxJH340ubz+1v/p2LHv6IpdtrYl1UIYQQfZy0jI+TSZmYkTODGTkz2Neyj1e3vsorW17BY32aXYHXmfeP6ZyWcT4/P386IzMSY11cIYQQfZC0jHtRVkIWt0y6hfcuf48HzniAqbkjsWcsYqX+L7760s185+WXOdDoiXUxhRBC9DESxieA1WTlnIJz+Nt5T/LPS/7J10ZdSVzSFv7Teg9nvXgR8154lk+2VxMKSdeaQgghehjGSqnzlFKblVLblFJ3HGa7rymltFKqpPeK2L8Ndw3n7pl38vE3FnNr4U9JcIRY6b+P69++lS89+Ap/fH8Lu2tbY11MIYQQMXTEMFZKmYGHgfOB8cBVSqnxUbZLBL4PfNbbhRwI4q3xfHvKN1jyzbe4ufhW4l3baE77DX9Z9Qiz7n+Pq+Z/yj9WVtDmC8a6qEIIIU6ynrSMpwPbtNZlWmsf8CJwcZTtfgn8DpCToodhN9v57qRv89ZlbzCn4Ezs6e+TPeH/2NH2GbcvWMW0X73PHa+uYUV5rYwQJYQQg0RPwjgX2N3heUV4WYRSagqQp7V+qxfLNqBlO7N5YPYDPHHOE2QlJtGS/ASnzvgHs8aHeH31Hr72yFLOfuBDHl68jX0N8vtGCCEGMnWk1pdS6nLgPK31DeHn1wCnaK1vDT83AR8A12qtdyqlSoH/1lovj7Kvm4CbADIzM6e++OKLvfZBmpubcTqdvba/kymogyxpWsLb9W/j135mOWfjbj2bT/dY2FIXQgET0sxMyzQzOcNCkr3nHYn053o5kaReopN6iU7qJTqpl+i6q5czzzxzhdY66jVVPQnj04C7tdbnhp//FEBr/ZvwcxewHWgOvyQLqAUuihbI7UpKSvTy5d2uPmqlpaXMnj271/YXC9Vt1Ty08iFe2/YaGXEZ3F5yO+MTz+AfX1Ty2qpKdte2YVIwrSCFcydkce7ELHKT4w67z4FQLyeC1Et0Ui/RSb1EJ/USXXf1opTqNox70unH58AopdQwoBL4OvCN9pVa6wYgrcObldJNy1gcXlpcGr+c+UsuH305v/7s19zx0R1MyZjCnafcye1zzmTD3kYWrdvHovX7uefNDdzz5gaKhriMYJ6QxcgM+YUqhBD90RHDWGsdUErdCiwCzMCTWuv1Sql7gOVa69dPdCEHm+L0Yl644AUWblvIQysf4so3r+TSkZdyas6pXHHqBH44ZxY7qltYtH4/i9bv4/5Fm7l/0WZGZjg5LxzME3OTUEr6xRZCiP6gR91haq3fBt7usuwX3Ww7+/iLJcwmM5ePvpw5+XP48xd/5tWtr0aGcEy0JjIudRzjU8fz7QvGk2YbwfqdNhatP8AjH27nz4u3kZscx7kTssjwB5kZDGE1S/8uAK3+Vp7Z8Azv7nuXTz77hKL0IorTihmSOER+vAghYkb6pu7jXHYXPzv1Z/x42o/ZWr+VDTUb2FizkQ01G3hh4wv4Qj4AnFYnY4eOZd7EMfhbc9hekcxzn+3AF4CH1/yLL41MY/aYdM4YnUGWyxHjT3XyBUIBXtv2Gn9Z9Req2qoYYh3CP7f9k79v+jsAbrubovSiyGNi6kScNjnsL4Q4OSSM+wmr2cr41PGMTz3Y34o/5KesvowNNRtYX7OejbUbeb3sFbxBLyhwj4snLpgGKpGlzbD4UxN8YiU5Lp4hyS5GpLkpSEkmwRqHw+LAbrbjsDhwmB24HW4mpE7AbOrf4zJrrfmw4kP+sOIPlDWUMSl9Eg/OfpD6DfXMOn0W2+q3saZ6DWuqjMeHFR8CoFCMSB5BcXoxxenFFKUXMcw1DJOSIwxCiN4nYdyPWU1WxqSMYUzKGC4ddSlgtADLGsoireeVO1cSl2QjI9lDo7eZRk8rrQEPm1q8bGr1o3Z3fzV9iiOFLw/9MnMK5lCSWYLF1L++Lmur1vLgigdZvn85BUkF/HH2Hzlr6FkopSjdUIrZZI7U3xWjrwCg0dfI2qq1rKlaw+rq1bxX/l7k9IDT6qQks4Sbim6iML0wlh9NCDHA9K+/ruKILCYLo92jGe0ezcUjL6a0Lfol9k0eP//ZVs3izftYsrWCfc3NoPwUpFuZPDSBvEwP5W2f8UbZGyzYsoBkezJnDz2bOflzmJ49HavJevI/XA/tbtzNn774E+/ufJcURwp3nXIXl42+rEdlTrIlMTN3JjNzZwIQ0iHKG8uNcK5azb93/ZtvvP0N5uTP4bbJt1HgKjjBn0YIMRhIGA9SiQ4r503M5ryJ2Wg9ia0HmindfIAPt1Txxue1+IMJmNRZjMm+gNzsXbTZvuDtHe/w6tZXSbIlMTtvNufkn8NpOadhM9ti/XEAqPPUMX/NfF7c/CJWk5XvFH+HaydcS4I14Zj3aVImhrmGMcw1jItHXsx/lfwXT69/mqfWP8UHuz7ga6O+xneKv0N6fHovfhIhxGAjYSxQSjE6M5HRmYncdPoIWrwBlu2s5YvyOlbsquOTtdm0+jJAnUVq+k4caZt4d8e/eX376yRYnczOO4M5+XOYmTMTh6X7i8MCoQCtgVZa/a20Blpp87dFngdCAVx2FylxKaTYU0iyJ/X4/Kwn4OG5jc/x17V/pTXQymWjLuPm4ptPSEAmWBO4edLNXDnmSuavmc/Lm1/mjbI3uHrc1Vw38ToSbYm9/p5CiIFPwlgcIsFu4cwxGZw5JgOAQDDEpn1NrNxVx4ryfFaUF1FTdz7mhO0EXet4x1fKW2VvYTM5KE6fjNmkI4Hbcdp+5XdPmJWZZHsyboebVEcqboebFEdKZNo+v6txFw+vepj9rfuZPWQ2P5j6A0YkjzhRVRORFpfGnafcydXjrubPX/yZx9c+zstbXubGwhv5+tiv95mjBUKI/kHCWByRxWxiYq6Libku/t9pBQDsb/SwsryOFeV1LN9Vzca6L/AlrOXT1p3YTA4SbQmkxKcxNCGJ7CQXyXFOEiwJxFvjibfEHzI1KRP13npqPbXUeeqo9dR2mt9Yu5FaTy1NvqZDyjcxdSK/mfUbpmVNO8k1A0OThnLfGfcxb+I8HlrxEPcvv5/nNz7PrZNv5YJhF/T7q9GFECeHhLE4JplJDs4vzOb8wmwAPP6ZrKtsYOWuOtZUNLC2soHVNa2R7YelJVA0xEVhrosxQ5KZmJtEvO3ov37+oJ8678GwtigL07KmxbzDjgmpE5h/znw+2fMJf1zxR+78+E6eWv8UP5jyA76U+6WYl08I0bdJGIte4bCaKSlIoaQgJbKsvtV3MJh317NsRy3/XLUHAJOCkRlOioYkUzTEaHWPynCS6Dj8Fc9Ws5WM+Awy4jNO6Oc5VjNyZnBq9qks2rmIP638Ezf/+2ZKMkv4/pTvU5xeLKEshIhKwlicMMnxNk4fnc7pow9eSHWgycPaigZWVzSwtqKexZsO8MqKisj6bJeDkRlORmUkMirTGZ53khzff87BmpSJ84edz5eHfplXtr7Co6sf5Zp3rmG4azgXDr+QC4ZfQK4z98g7EkIMGhLG4qTKSHRw9jgHZ4/LBIwesvY0eFhX2cC2A82Rx9+X7aLNH4y8Ls1pZ1SGk1GZRjiPCAd2mtPWZ1ubVrOVq8ZexUUjLuKtsrd4q+wt/vTFn/jTF39iSsYUvjL8K5xbcC4uuyvWRRVCxJiEsYgppRS5yXHhgS0OLg+FNJX1bWw70MzWA03haTMLV1bS5A1EtnPHWxmfk8T47CTG5yQxIcfF8LQELH1oYIwEawJXjrmSK8dcSWVzJW+Xvc2bZW/yy09/yW+W/YZZubO4cPiFnJF3BnazPdbFPWla/a3UemrJceZIN6Ni0JMwFn2SyaTIS4knLyWeM8cePD+stWZ/ozcS0pv3NbFhbyNPLy3HFwgBYLeYGJuVaIR0jovx2UmMy048pgvGeluuM5cbi27khsIb2FS7iTfL3uSdHe+wePdinFYnc/LncOHwCynJKhlQAaW1pqK5gtVVq1l1YBWrq1azpW4LIR0i0ZZIYVohhWmFxiAdaRNJcaQceadCDCCx/+skxFFQSpHlcpDlcvClUWmR5YFgiLLqFtbvaWDDnkY27G3knXX7+Puy3eHXGVd0j882Ws/+qgDDa1rJdcdhNp38w9xKKcaljmNc6jhun3o7y/Yt482yN1m0cxELty0kIz6DC4ZdwFlDzyLVkUqiLRGnzdmnuyHtyBv0sqFmQyR4Vx1YRY2nBoB4SzxF6UXcWHgjGfEZbKjZwLrqdTy+9nFC2vhBlevMpSitiMJ0I6THpow9bIcyQvR3EsZiQLCYTZFexC6dbCzTWrO3wcP6PY3hgG5g1e563lyzF4AHVyzGalYMTYlnWFoCw9ISKAhPh6UlkJnowHQSgtpsMnNazmmclnMad516Fx/u/pA3y97kuQ3P8dT6pzptG2eJI9FqBHOiLdF4WBMj806bkyRbEnaznbZAG22Bg72ctc9Hej4LhJeF17UF2vAFfLhecuGyu0iyJR1xmmRPwmVz4Q/5WVO1hlVVq1h9YDUbajcQCBmnE/IS85iRM4NJGZMoTi9mZPLIqPdft/pb2VCzgbXVa1lbvZYvqr7gnZ3vAGBRFkanjI60oKdkTCEvKe+E/9sMBk2+Juo8deQ4c/rdYDADidS8GLCUUuQkx5GTHMec8ZmR5Q1tfl56dwnJeaPZUd3CzuoWdlS38NHWarzhQ90ADquJgtSEQ4I6PyWe9ET7CblwLM4Sx3nDzuO8YedR56ljTdUaGn2NNPmaIo9mf3NkWZ2njt1Nu2nyNdHoa4wEYLT9xlvijanVmCZYEkiPS++0rrKiEneWmwZvAw2+Bqpaq9hev51GbyNN/kM7XOnKbrYzIXUC14y/hknpRvimxqX26LPHW+MpySqhJKsksqyqtYo11WtYV72OtVVrebPsTV7a/BIAY1PGcm7BuZybf26fC2Z/yM+qA6uoaKpgRPIIRrlHEWeJi3WxCOkQZfVlrK5aHRk6dHv9djQam8lGgauAkckjGZk8khHJIxiZPJJcZ650XnMSSBiLQccVZ2W028zsks5/wEMhzb5GDzvC4dwe1Jv3NfGvDfsJhA4ON+mwmshzxzM0fF57aPsjNZ48dzxxtuP/4+V2uDkj74web6+1xhv00uRrwhP0RELWYXH0+PxzaUsps0+bHXVdIBSg2ddMg6+BBm8Djb7GyFRrHTmcbDX33qH09Ph0zh56NmcPPRuAYCjIjoYdLN27lEU7F/HQyod4aOVDjE8dz3kF53FOwTkxu22s3lPPR5UfsaRiCf+p/E+nHy8mZSI/KZ+x7rGRYTvHpowlLS7tMHvsnTKtqTZGHFtTZfyoafY3A+CyuyhKK+LcgnPJiM9gR8MOttVv44sDX/D2jrcj+3CYHQxzDesU0COSR5DjzDmhZR9sJIyFCDOZDrakZ47s/EcyEAxRUdfGjpoWKmpb2RV5tPFpWQ0tvmCn7dOcdoamxEVCuj2w81MTyEi0n5DD30opHBbHCTu3ajFZSHYkk+xIPiH77wmzycxI90hGukdyzfhr2Nu8l/fK3+PdHe/y4IoHeXDFgxSlFXFOwTmcW3AuWQlZJ6wsWmu21W/jw4oPWVKxhNVVqwnpEKmOVL6c/2XOyDuDEa4RbG/YzubazWyq3cTqqtWRQ+8AqY5UxqaMZXTKaMa6xzI2ZSz5SflHbIlqrQnqIP6Qn0AoQCAUwB/yU91WzZoqo8W7pnoN5Y3lgPFjYLR7NBcMu4DijGKK0orIT8rv9uhOi7+F7fXb2Va/jW3129hev53P9hlDqraLs8SRolJ47r3nSLYnd3q47K5I3/Lt806rs8/ehtgXSBgL0QMWs4mC8KHqrrTW1LX6IwG9u7aVXTXG/Oc763h99R46NKqxWUzkueMi4dypZZ3SO63qwSLbmc28CfOYN2EeFU0VLNq5iEU7F/H75b/n98t/z6T0SZw37Dzm5M/plV7bvEEvy/ctp3R3KUsqlrCnxehRblzKOG4quokzhpzB+NTxnY5EFLgKIi17gAZvA1vqtkQCekvdFp7d8GzkFIPD7CDbmU1Ih/AHw2GrA5HgbZ8eToojheL0Yi4ZeQnF6cVMSJ1AvDW+x58zwZpAUXoRRelFnZY3+hopqy9ja/1WttdvZ/XO1XgCHja3bKbeW0+DtwGNjrpPi7KQZE/CbXeTFp/GaPdoxriNowQjXCN69YhKfyRhLMRxUkqRkmAjJcHGpLzkQ9b7AiEq69vYXdtKeZSwbvZ2/sOanmjv1KIe4o5jiDuOPHc8WS4H1j50D3VfMiRxCN8q/BbfKvwW5Y3lkWD+7bLf8rtlv2NK5hRm5MzAbrZjVmbMJjNmZcakTJHnJmXCoiyHLPuk6RMWfrCQpXuX0hZow2F2cGrOqdxYdCOzcmeRmZB55AKGuewupmVN6zSwiT/op6yhjM11m9lcu5m9LXuxmCxYTVYsJgsWZcFqtmJRFuN5+BFZH55PsiUxMW0iuc7cE9IKTbIlMSljEpMyJgFQ2lrK7NmzI+uDoSBNvibqvfWRcG6f77hsT/MeFmxegDfoBYyjLsNdwyPhPCZlDGPcY3A73EdVPq01Lf4WqtuqqfHUGNO2Glr8LVhNVqxmqzHtMm8z2w4u77DOZraRl3hyrkeQMBbiBLNZTJGLwLqK1qour2lhV20ry3bU8tqqSnSHhoZJQVaSgyHugyGd646LPM92xWGzSFjnJ+VzU9FN3FR0E2UNZUYw71jE/33xf8e8zyxvFheNuIjTh5zO9KzpvXo6wGq2RkKIEz8C6AljNpl7fCojEAqwq3FX5AfI5rrNfLa386HwjLgMRqcYLeixKWNJj0+n1lNLTVtNJHBr2sKPcPi2B3xvSLQm8sk3Pum1/R2OhLEQMdSTVvW+Bg8Vda1U1LUZ0/o2Kura+GxHLa+taut0CFwpyEx0kJcSR374SvD81HgKUo1D7E774PsvP9w1nO8Wf5fvFn+XVn8rQR0kpEMEQgFCOkRQB41loRAB3WFZKBiZX//Fer4+5+tyzrMXWUwWhicPZ3jycM4fdn5kea2nls21myOH8jfXbebTPZ8S0J2PICmUMd55XCqpjlSGJg0l1ZFKWlyasSy8PDUuFafVaZxjD/rxh4yHL+iLzPtD/k7r2h+Kk/fvPfj+ZwrRj9gsJoamGldpR+MPtoe1EdSV4aDeVdvKR1urOg3CAcaFZcPSDoZzQWo4rAdJUB/NedOOam21EsQnSYojJXLffbv2w/g1bTWkxKWQFpdGsj356O+L7sOnpQf+/z4hBjCr2RTpNhQOvZ+31RdgZ7Vx6HtHjXGr1s7qVj7cUsXLXYI6PdGOy+xnQeUKMhIdpCfayUi0k5HkMKaJdtzxtpPSEYoQHbUfxh/IJIyFGMDibZZwH91Jh6xr8QbYWdNCeU1r5J7qtTv2sGlfEx9tqe40IEc7q1mR5uwc0BmJDjKT7Axxx5PrjiMn2YHdIleEC3E0JIyFGKQS7BYm5LiYkHNwCMfS0rrI1bFtviAHmjwcaPJyoNHLgSYP+8PTqiYvu2paWb6zlrpW/yH7zki0Ry4sy00+eKFZnjuO3GS5fUuIriSMhRBRxdnM5KcmkJ966FXgHfkCIfY3eiLnqyvr2qisNy44W1NRz7vr9uIPdr73NDXBRq47jmyXg5QEOykJVtzxtsjFbCkJtsjzeJtZzteKAU/CWAhxXGyWjuetDxUMaaqavJ0uMKuoa6Oyvo0d1S2sKK+nrtVHMBS9swibxURKvA13go3UhIPT3OS48GFxYzzsNKdNQlv0WxLGQogTymw6OOxlSTfbhEKaJm+A2hYftS0+6lp81LZ2mYYflfVtVDV5D+ksxWYxGQGdbJy3zk2ON6buOIYkGx2myD3Yoq+SMBZCxJzJpHDFWXHFWaN2jtKV1ppGT4DKujb21But7Mijro3SzVUcaOrc+YNSxq1d6U47aYl20pw20hON5+mJdtKcxiM90U5ynFWuGhcnlYSxEKLfUepgeEe7UhzAGwiyt97Dnvo2KuqN0N5b76G62Ut1s5dt+5uobvbhC4YOea3FpEh12iIBHWj2srR1I6lOG6kJdlKcNtIS7KQ6jfPaDqtckCaOj4SxEGJAslvM3Q7u0U5rTWNbgKpmL1VN3khQt88bUx+VtUE+/8/OqMEN4LRbIsGcmmAnNcFmBLfTaIGnhoO7/Zy39C8uupIwFkIMWkopXPFWXPFWRmY4u92utLSUM844g2ZvgJpmHzUtPmqavdSEz2NXN3upaTbmK+paWV1RT21L9xelJcdbw4F9aFinOu2kJNgiLf+kOCsJckX5gNenwtjv91NRUYHH4znq17pcLjZu3HgCStW/HU+9OBwOhgwZgtXah/uQE+IkUUqR6LCS6LAetrXdLhTSNLT5I4Fd0+yluj3Em33UtBit7i37m6lprol6v3Y7i0mR1CGcXZGHpcO8cXtYZpJxsVya045Zznv3G30qjCsqKkhMTKSgoOCofwU2NTWRmJh4gkrWfx1rvWitqampoaKigmHDhp2AkgkxsJlMCnf4sHRP+IMh6lp9kRZ2Y5ufhsM8dte2RuajtcDNJkVGot0I53BAZ7mM+cwkB9nh53K+u2/oU2Hs8XiOKYhF71NKkZqaSlVVVayLIsSgYDWbyEh0kJF4dEMzaq1p8QVpaPNT2+xjX6OHfY0e9jcY030NHrZVNfOfbdG7OHXFWY1+yeOsJDksJMVZSXJYSYqzhKfRnhvbid7Tp8IYkCDuQ+TfQoi+TymF027BabeQmxxHIa5ut232BtjX4GF/OKTbw7qqyUuT1091s4+y6hYa2/w0egLdnvNu5zBD5ueLO90e1nlqizyXFvjh9bkwjjWn00lzc3OsiyGEEL3OabcwMsN52IvV2mmtafUFafT4aWwLhKf+g8/b/KzdUoYjOZmqJi/bDjSztKyG+m7OfSc6LAeD2mnHHe4Ctb3b0+R4a6QbVHeCbdBdtCZhLIQQ4hBKKRLsFhLsFrK7aWyXmiuZPXtyp2W+QIialkNvD6tq8kZuIdu0r5H6Vj91rT66a3zbzKaoge2OPzh1J1hJDq93xxuH0/trZy0Sxt3QWvPjH/+Yd955B6UUd911F3PnzmXv3r3MnTuXxsZGAoEAjzzyCDNmzOBb3/oWy5cvRynF9ddfzw9/+MNYfwQhhDjpbBYT2a44sl1xR9w2FNI0evxGF6it/kj3p/WtPmpbOj/ftK+RulY/9YcJcJMiclX5weA25tvPibvCod1+ZXr7vMNqimlLvM+G8f++sZ4Nexp7vH0wGMRsPvw5ifE5SfzPVyf0aH//+Mc/WLVqFatXr6a6uppp06Zx+umn88ILL3Duuefys5/9jGAwSGtrK6tWraKyspJ169YBUF9f3+NyCyHEYGUyqXBY9uyKcwj3Y+4JUNfqo67VF2lhtwd1x/m9DR427m2kvs1Pqy942P1azSoc2OGQjrOS5rTx4JWTjvNT9kyfDeNY+/jjj7nqqqswm81kZmZyxhln8PnnnzNt2jSuv/56/H4/l1xyCZMmTWL48OGUlZXxve99j6985Succ845sS6+EEIMSCbTwY5aCjjy/d7t/MFQ5MK09tvGGj3haficuDF/8PaxZk/39373tj4bxj1twbY7WfcZn3766SxZsoS33nqLa6+9lttvv53/9//+H6tXr2bRokU8+uijLFiwgCeffPKEl0UIIUTPWM0mUp12Up32WBclKukgtRuzZs3ipZdeIhgMUlVVxZIlS5g+fTrl5eVkZmZy4403csMNN7By5Uqqq6sJhUJ87Wtf495772XlypWxLr4QQoh+pM+2jGPt0ksvZenSpRQXF6OU4r777iMrK4unn36a+++/H6vVitPp5JlnnqGyspLrrruOUMjoRP43v/lNjEsvhBCiP+lRGCulzgMeAszAE1rr33ZZfztwAxAAqoDrtdblvVzWk6L9HmOlFPfffz/3339/p/Xz5s1j3rx5h7xOWsNCCCGO1REPUyulzMDDwPnAeOAqpdT4Lpt9AZRorYuAV4D7erugQgghxEDVk3PG04FtWusyrbUPeBG4uOMGWuvFWuvW8NNPgSG9W0whhBBi4OrJYepcYHeH5xXAKYfZ/lvAO9FWKKVuAm4CyMzMpLS0tNN6l8tFU1NTD4p0qGAweMyvHciOt148Hs8h/04DQXNz84D8XMdL6iU6qZfopF6iO5Z66dULuJRSVwMlwBnR1mut5wPzAUpKSvTs2bM7rd+4ceMx354kQyhGd7z14nA4mDx58pE37GdKS0vp+v0TUi/dkXqJTuolumOpl56EcSWQ1+H5kPCyTpRSXwZ+BpyhtfYeVSmEEEKIQawn54w/B0YppYYppWzA14HXO26glJoMPAZcpLU+0PvFFEIIIQauI4ax1joA3AosAjYCC7TW65VS9yilLgpvdj/gBF5WSq1SSr3eze6EEEII0UWPzhlrrd8G3u6y7Bcd5r/cy+Ua8AKBABaL9LkihBBCusOM6pJLLmHq1KlMmDCB+fPnA/Duu+8yZcoUiouLOfvsswHjirnrrruOwsJCioqKePXVVwFwOg8O3P3KK69w7bXXAnDttdfyne98h1NOOYUf//jHLFu2jNNOO43JkyczY8YMNm/eDBhXQP/3f/83EydOpKioiP/7v//jgw8+4JJLLons91//+heXXnrpSagNIYQQJ1rfbZq9cwfsW9vjzeOCATAf4eNkFcL5vz38NsCTTz5JSkoKbW1tTJs2jYsvvpgbb7yRJUuWMGzYMGprawH45S9/icvlYu1ao5x1dXVH3HdFRQWffPIJZrOZxsZGPvroIywWC++//z533nknr776KvPnz2fnzp2sWrUKi8VCbW0tbrebm2++maqqKtLT0/nb3/7G9ddff+SKEUII0ef13TCOoT/96U8sXLgQgN27dzN//nxOP/10hg0bBkBKSgoA77//Pi+++GLkdW63+4j7vuKKKyLjLjc0NDBv3jy2bt2KUgq/3x/Z73e+853IYez297vmmmt47rnnuO6661i6dCnPPPNML31iIYQQsdR3w7gHLdiO2nrpPuPS0lLef/99li5dSnx8PLNnz2bSpEls2rSpx/tQSkXmPR5Pp3UJCQfH3/z5z3/OmWeeycKFC9m5c+cR70u77rrr+OpXv4rD4eCKK66Qc85CCDFAyDnjLhoaGnC73cTHx7Np0yY+/fRTPB4PS5YsYceOHQCRw9Rz5szh4Ycfjry2/TB1ZmYmGzduJBQKRVrY3b1Xbm4uAE899VRk+Zw5c3jssccIBAKd3i8nJ4ecnBzuvfderrvuut770EIIIWJKwriL8847j0AgwLhx47jjjjs49dRTSU9PZ/78+Vx22WUUFxczd+5cAO666y7q6uqYOHEixcXFLF68GIDf/va3XHjhhcyYMYPs7Oxu3+vHP/4xP/3pT5k8eXIkeAFuuOEGhg4dSlFREcXFxbzwwguRdd/85jfJy8tj3LhxJ6gGhBBCnGxynLMLu93OO+9E7Vqb888/v9Nzp9PJ008/fch2l19+OZdffvkhyzu2fgFOO+00tmzZEnl+7733AmCxWHjwwQd58MEHD9nHxx9/zI033njEzyGEEKL/kDDuR6ZOnUpCQgIPPPBArIsihBCiF0kY9yMrVqyIdRGEEEKcAHLOWAghhIgxCWMhhBAixiSMhRBCiBiTMBZCCCFiTMJYCCGEiDEJ4+PQcXSmrnbu3MnEiRNPYmmEEEL0VxLGQgghRIz12fuMf7fsd2yq7fngDMFgMDIaUnfGpozlJ9N/0u36O+64g7y8PG655RYA7r77biwWC4sXL6aurg6/38+9997LxRdf3ONygTFYxHe/+12WL18e6V3rzDPPZP369Vx33XX4fD5CoRCvvvoqOTk5XHnllVRUVBAMBvn5z38e6X5TCCHEwNRnwzgW5s6dyw9+8INIGC9YsIBFixZx2223kZSURHV1NaeeeioXXXRRp5GZjuThhx9GKcXatWvZtGkT55xzDlu2bOHRRx/l+9//Pt/85jfx+XwEg0HefvttcnJyeOuttwBjMAkhhBADW58N48O1YKNp6oUhFCdPnsyBAwfYs2cPVVVVuN1usrKy+OEPf8iSJUswmUxUVlayf/9+srKyerzfjz/+mO9973sAjB07lvz8fLZs2cJpp53Gr371KyoqKrjssssYNWoUhYWF/Nd//Rc/+clPuPDCC5k1a9ZxfSYhhBB9n5wz7uKKK67glVde4aWXXmLu3Lk8//zzVFVVsWLFClatWkVmZuYhYxQfq2984xu8/vrrxMXFccEFF/DBBx8wevRoVq5cSWFhIXfddRf33HNPr7yXEEKIvqvPtoxjZe7cudx4441UV1fz4YcfsmDBAjIyMrBarSxevJjy8vKj3uesWbN4/vnnOeuss9iyZQu7du1izJgxlJWVMXz4cG677TZ27drFmjVrGDt2LCkpKVx99dUkJyfzxBNPnIBPKYQQoi+RMO5iwoQJNDU1kZubS3Z2Nt/85jf56le/SmFhISUlJYwdO/ao93nzzTfz3e9+l8LCQiwWC0899RR2u50FCxbw7LPPYrVaycrK4s477+Tzzz/nRz/6ESaTCavVyiOPPHICPqUQQoi+RMI4irVr10bm09LSWLp0adTtmpubu91HQUEB69atA8DhcPC3v/3tkG3uuOMO7rjjjk7Lzj33XM4999xjKbYQQoh+Ss4ZCyGEEDEmLePjtHbtWq655ppOy+x2O5999lmMSiSEEKK/kTA+ToWFhaxatSrWxRBCCNGPyWFqIYQQIsYkjIUQQogYkzAWQgghYkzCWAghhIgxCePjcLjxjIUQQoiekjAeAAKBQKyLIIQQ4jj02Vub9v3613g39nw840AwSO0RxjO2jxtL1p13dru+N8czbm5u5uKLL476umeeeYbf//73KKUoKiri2WefZf/+/XznO9+hrKwMgEceeYScnBwuvPDCSE9ev//972lububuu+9m9uzZTJo0iY8//pirrrqK0aNHc++99+Lz+UhNTeX5558nMzOT5uZmbrvtNpYvX45Siv/5n/+hoaGBNWvW8Mc//hGAxx9/nA0bNvCHP/zhiJ9LCCFE7+uzYRwLvTmescPhYOHChYe8bsOGDdx777188sknpKWlUVtbC8Btt93GGWecwcKFCwkGgzQ3N1NXV3fY9/D5fCxfvhyAuro6Pv30U5RSPPHEE9x333088MAD3HfffbhcrkgXn3V1dVitVn71q19x//33Y7Va+dvf/sZjjz12vNUnhBDiGPXZMD5cCzaavjaesdaaO++885DXffDBB1xxxRWkpaUBkJKSAsAHH3zAM888A4DZbMblch0xjOfOnRuZr6ioYO7cuezduxefz8ewYcMAKC0tZcGCBZHt3G43AGeddRZvvvkm48aNw+/3U1hYeJS1JYQQorf02TCOlfbxjPft23fIeMZWq5WCgoIejWd8rK/ryGKxEAqFIs+7vj4hISEy/73vfY/bb7+diy66iNLSUu6+++7D7vuGG27g17/+NWPHjuW66647qnIJIYToXXIBVxdz587lxRdf5JVXXuGKK66goaHhmMYz7u51Z511Fi+//DI1NTUAkcPUZ599dmS4xGAwSENDA5mZmRw4cICamhq8Xi9vvvnmYd8vNzcXgKeffjqy/Mwzz+Thhx+OPG9vbZ9yyins3r2bF154gauuuqqn1SOEEOIEkDDuItp4xsuXL6ewsJBnnnmmx+MZd/e6CRMm8LOf/YwzzjiD4uJibr/9dgAeeughFi9eTGFhIVOnTmXDhg1YrVZ+8YtfMH36dObMmXPY97777ru54oormDp1auQQOMCPfvQj6urqmDhxIsXFxSxevDiy7sorr2TmzJmRQ9dCCCFiQw5TR9Eb4xkf7nXz5s1j3rx5nZZlZmbyz3/+85Btb7vtNm677bZDlpeWlnZ6fvHFF0e9ytvpdHZqKXf08ccf88Mf/rC7jyCEEOIkkZbxIFRfX8/o0aOJi4vj7LPPjnVxhBBi0JOW8XHqj+MZJycns2XLllgXQwghRJiE8XGS8YyFEEIcrz53mFprHesiiDD5txBCiJOjT4Wxw+GgpqZGQqAP0FpTU1ODw+GIdVGEEGLA61OHqYcMGUJFRQVVVVVH/VqPxyPBEcXx1IvD4WDIkCG9XCIhhBBd9SiMlVLnAQ8BZuAJrfVvu6y3A88AU4EaYK7WeufRFsZqtUa6cTxapaWlTJ48+ZheO5BJvQghRN93xMPUSikz8DBwPjAeuEopNb7LZt8C6rTWI4E/AL/r7YIKIYQQA1VPzhlPB7Zprcu01j7gRaBr7xIXA+09S7wCnK2ONKyREEIIIYCehXEusLvD84rwsqjbaK0DQAOQ2hsFFEIIIQa6k3oBl1LqJuCm8NNmpdTmXtx9GlDdi/sbKKReopN6iU7qJTqpl+ikXqLrrl7yu3tBT8K4Esjr8HxIeFm0bSqUUhbAhXEhVyda6/nA/B6851FTSi3XWpeciH33Z1Iv0Um9RCf1Ep3US3RSL9EdS7305DD158AopdQwpZQN+DrwepdtXgfaRz64HPhAy83CQgghRI8csWWstQ4opW4FFmHc2vSk1nq9UuoeYLnW+nXgr8CzSqltQC1GYAshhBCiB3p0zlhr/Tbwdpdlv+gw7wGu6N2iHbUTcvh7AJB6iU7qJTqpl+ikXqKTeonuqOtFydFkIYQQIrb6VN/UQgghxGA0IMJYKXWeUmqzUmqbUuqOWJenr1BK7VRKrVVKrVJKLY91eWJFKfWkUuqAUmpdh2UpSql/KaW2hqfuWJYxFrqpl7uVUpXh78wqpdQFsSxjLCil8pRSi5VSG5RS65VS3w8vH9TfmcPUy6D+ziilHEqpZUqp1eF6+d/w8mFKqc/CufRS+ALo7vfT3w9Th7vr3ALMweiQ5HPgKq31hpgWrA9QSu0ESrTWg/o+QKXU6UAz8IzWemJ42X1Ardb6t+EfcG6t9U9iWc6TrZt6uRto1lr/PpZliyWlVDaQrbVeqZRKBFYAlwDXMoi/M4eplysZxN+ZcG+TCVrrZqWUFfgY+D5wO/APrfWLSqlHgdVa60e6289AaBn3pLtOMYhprZdgXOXfUccuXJ/G+KMyqHRTL4Oe1nqv1npleL4J2IjRy+Cg/s4cpl4GNW1oDj+1hh8aOAuje2jowfdlIIRxT7rrHKw08J5SakW49zNxUKbWem94fh+QGcvC9DG3KqXWhA9jD6pDsV0ppQqAycBnyHcmoku9wCD/ziilzEqpVcAB4F/AdqA+3D009CCXBkIYi+59SWs9BWPErVvChyVFF+EOavr3+Zre8wgwApgE7AUeiGlpYkgp5QReBX6gtW7suG4wf2ei1Mug/85orYNa60kYPVROB8Ye7T4GQhj3pLvOQUlrXRmeHgAWYnxJhGF/+BxY+7mwAzEuT5+gtd4f/sMSAh5nkH5nwuf+XgWe11r/I7x40H9notWLfGcO0lrXA4uB04DkcPfQ0INcGghh3JPuOgcdpVRC+CILlFIJwDnAusO/alDp2IXrPOCfMSxLn9EeNmGXMgi/M+ELcv4KbNRaP9hh1aD+znRXL4P9O6OUSldKJYfn4zAuJt6IEcqXhzc74vel319NDRC+lP6PHOyu81exLVHsKaWGY7SGwehp7YXBWi9Kqb8DszFGUtkP/A/wGrAAGAqUA1dqrQfVxUzd1MtsjMONGtgJfLvDedJBQSn1JeAjYC0QCi++E+P86KD9zhymXq5iEH9nlFJFGBdomTEauAu01veE/wa/CKQAXwBXa6293e5nIISxEEII0Z8NhMPUQgghRL8mYSyEEELEmISxEEIIEWMSxkIIIUSMSRgLIYQQMSZhLIQ4hFJqtlLqzViXQ4jBQsJYCCGEiDEJYyH6MaXU1eGxVFcppR4Ld1jfrJT6Q3hs1X8rpdLD205SSn0a7tB/YXuH/kqpkUqp98Pjsa5USo0I796plHpFKbVJKfV8uAcmIcQJIGEsRD+llBoHzAVmhjupDwLfBBKA5VrrCcCHGD1rATwD/ERrXYTRi1L78ueBh7XWxcAMjM7+wRiV5wfAeGA4MPMEfyQhBi3LkTcRQvRRZwNTgc/DjdY4jMELQsBL4W2eA/6hlHIByVrrD8PLnwZeDvdfnqu1XgigtfYAhPe3TGtdEX6+CijAGDhdCNHLJIyF6L8U8LTW+qedFir18y7bHWuftx370Q0ify+EOGHkMLUQ/de/gcuVUhkASqkUpVQ+xv/r9tFivgF8rLVuAOqUUrPCy68BPtRaNwEVSqlLwvuwK6XiT+aHEELIL10h+i2t9Qal1F3Ae0opE+AHbgFagOnhdQcwziuDMYzbo+GwLQOuCy+/BnhMKXVPeB9XnMSPIYRARm0SYsBRSjVrrZ2xLocQoufkMLUQQggRY9IyFkIIIWJMWsZCCCFEjEkYCyGEEDEmYSyEEELEmISxEEIIEWMSxkIIIUSMSRgLIYQQMfb/AWP/P725MK7tAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# plotting the measurments in the history object\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation error is computed at the end of each epoch, while the training error is computed using a running mean during each epoch. So the training curve should be shifted by half an epoch to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 57.4205 - accuracy: 0.8533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[57.42045593261719, 0.8532999753952026]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the evaluate() method\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "X_pred3 = X_test[:3]\n",
    "y_proba = model.predict(X_pred3)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_pred3)\n",
    "print(y_pred)\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[np.argmax(y_proba, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output layer has a single neuron with no activation function, the loss is mse.\n",
    "model = keras.models.Sequential([\n",
    "    # the data is noisy, to avoid overfitting we only use one neuron\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8075 - val_loss: 0.5327\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6633 - val_loss: 0.4751\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5335 - val_loss: 0.4278\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.4499 - val_loss: 0.4008\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.4361 - val_loss: 0.3937\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.3815\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.3792\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.3696\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6253 - val_loss: 0.3918\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4319 - val_loss: 0.3807\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4083 - val_loss: 0.3747\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.3670\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3647\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3900 - val_loss: 0.3600\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.3566\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.3532\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3793 - val_loss: 0.3527\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3774 - val_loss: 0.3583\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3750 - val_loss: 0.3498\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3727 - val_loss: 0.3495\n",
      "162/162 [==============================] - 0s 769us/step - loss: 0.3641\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3966956],\n",
       "       [3.3373933],\n",
       "       [2.1594954]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predr = X_test[:3]\n",
    "y_pred = model.predict(X_predr)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex models using the functional API\n",
    "\n",
    "One example of a nonsequential neural network is the following:\n",
    "<center><img src=\"img/non_sequential.png\"></img></center>\n",
    "It is used for learning the deep patterns as well as the simple rules, through the short path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(15, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1425.9247 - val_loss: 26.4639\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 998us/step - loss: 271.1783 - val_loss: 728.6556\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5944.3735 - val_loss: 21022.8184\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 292344.9688 - val_loss: 644968.9375\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 9082048.0000 - val_loss: 18387888.0000\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 246448928.0000 - val_loss: 549481920.0000\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 8305761792.0000 - val_loss: 15059102720.0000\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 147085656064.0000 - val_loss: 436059865088.0000\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4437341896704.0000 - val_loss: 12666442612736.0000\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 105981061103616.0000 - val_loss: 369061707907072.0000\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2980880712728576.0000 - val_loss: 10782369367719936.0000\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 114061815847059456.0000 - val_loss: 312114414406860800.0000\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3060139321949421568.0000 - val_loss: 9090544984905482240.0000\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 136582944087357784064.0000 - val_loss: 269633992943508389888.0000\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2010755058204380495872.0000 - val_loss: 7814134351905624686592.0000\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 80742083357770067738624.0000 - val_loss: 224607808043066633224192.0000\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3441012693183733153398784.0000 - val_loss: 6703528349641977347702784.0000\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 53326327383625488259874816.0000 - val_loss: 195520600658853763521445888.0000\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1565271019619442322562875392.0000 - val_loss: 5587392456623206421476933632.0000\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 50483802476176823276332384256.0000 - val_loss: 160731617868373064686789197824.0000\n",
      "162/162 [==============================] - 0s 779us/step - loss: 3455736820496080366537336160256.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "nonsequential = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a subset of features through the wide path and a different subset through the deep path.\n",
    "<center><img src=\"img/non_sequential2.png\"></img></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least name he most important layers\n",
    "inputA = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "inputB = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(inputB)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([inputA, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[inputA, inputB], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8139 - val_loss: 0.7748\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7349 - val_loss: 0.6336\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6428 - val_loss: 0.5824\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6032 - val_loss: 0.5507\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5771 - val_loss: 0.5290\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5572 - val_loss: 0.5113\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5401 - val_loss: 0.4965\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5288 - val_loss: 0.4847\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.4794\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5088 - val_loss: 0.4676\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5021 - val_loss: 0.4643\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4950 - val_loss: 0.4567\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4900 - val_loss: 0.4507\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4879 - val_loss: 0.4465\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4811 - val_loss: 0.4445\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4759 - val_loss: 0.4445\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4708 - val_loss: 0.4353\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4680 - val_loss: 0.4336\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4648 - val_loss: 0.4289\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4636 - val_loss: 0.4275\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, \n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 895us/step - loss: 0.4504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.4145455],\n",
       "       [2.5913994],\n",
       "       [2.5508673]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple outputs\n",
    "Examples:\n",
    "- Locate and classify objects\n",
    "- Independent tasks, a single ANN is better because it can learn useful features across tasks\n",
    "- Like a regularization technique \n",
    "<center><img src=\"img/non_sequential3.png\"></img></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "inputB = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(inputB)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([inputA, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[inputA, inputB], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of losses, if we only pass one, the same is used for both outputs.\n",
    "# We care much more for the main output loss, so lets declare its weight to 90%\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 1.2195 - main_output_loss: 1.1674 - aux_output_loss: 1.6886 - val_loss: 0.7318 - val_main_output_loss: 0.6800 - val_aux_output_loss: 1.1981\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6830 - main_output_loss: 0.6362 - aux_output_loss: 1.1037 - val_loss: 0.5406 - val_main_output_loss: 0.4900 - val_aux_output_loss: 0.9957\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5940 - main_output_loss: 0.5511 - aux_output_loss: 0.9801 - val_loss: 0.4915 - val_main_output_loss: 0.4415 - val_aux_output_loss: 0.9412\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5113 - main_output_loss: 0.4710 - aux_output_loss: 0.8744 - val_loss: 0.4621 - val_main_output_loss: 0.4270 - val_aux_output_loss: 0.7776\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4808 - main_output_loss: 0.4506 - aux_output_loss: 0.7528 - val_loss: 0.4382 - val_main_output_loss: 0.4101 - val_aux_output_loss: 0.6908\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4404 - main_output_loss: 0.4148 - aux_output_loss: 0.6699 - val_loss: 0.4040 - val_main_output_loss: 0.3790 - val_aux_output_loss: 0.6286\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4295 - main_output_loss: 0.4074 - aux_output_loss: 0.6285 - val_loss: 0.4012 - val_main_output_loss: 0.3794 - val_aux_output_loss: 0.5974\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4229 - main_output_loss: 0.4024 - aux_output_loss: 0.6076 - val_loss: 0.3909 - val_main_output_loss: 0.3709 - val_aux_output_loss: 0.5710\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4158 - main_output_loss: 0.3976 - aux_output_loss: 0.5795 - val_loss: 0.4017 - val_main_output_loss: 0.3829 - val_aux_output_loss: 0.5712\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4074 - main_output_loss: 0.3899 - aux_output_loss: 0.5648 - val_loss: 0.3745 - val_main_output_loss: 0.3567 - val_aux_output_loss: 0.5344\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3988 - main_output_loss: 0.3821 - aux_output_loss: 0.5494 - val_loss: 0.3728 - val_main_output_loss: 0.3559 - val_aux_output_loss: 0.5254\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3994 - main_output_loss: 0.3839 - aux_output_loss: 0.5386 - val_loss: 0.3774 - val_main_output_loss: 0.3620 - val_aux_output_loss: 0.5165\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3947 - main_output_loss: 0.3794 - aux_output_loss: 0.5328 - val_loss: 0.3673 - val_main_output_loss: 0.3514 - val_aux_output_loss: 0.5101\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3898 - main_output_loss: 0.3754 - aux_output_loss: 0.5195 - val_loss: 0.3611 - val_main_output_loss: 0.3460 - val_aux_output_loss: 0.4963\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3846 - main_output_loss: 0.3705 - aux_output_loss: 0.5121 - val_loss: 0.3559 - val_main_output_loss: 0.3411 - val_aux_output_loss: 0.4886\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3813 - main_output_loss: 0.3672 - aux_output_loss: 0.5081 - val_loss: 0.3534 - val_main_output_loss: 0.3392 - val_aux_output_loss: 0.4811\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - main_output_loss: 0.3634 - aux_output_loss: 0.4962 - val_loss: 0.3541 - val_main_output_loss: 0.3406 - val_aux_output_loss: 0.4755\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3752 - main_output_loss: 0.3622 - aux_output_loss: 0.4926 - val_loss: 0.3488 - val_main_output_loss: 0.3352 - val_aux_output_loss: 0.4704\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3737 - main_output_loss: 0.3610 - aux_output_loss: 0.4877 - val_loss: 0.3491 - val_main_output_loss: 0.3356 - val_aux_output_loss: 0.4707\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3683 - main_output_loss: 0.3561 - aux_output_loss: 0.4782 - val_loss: 0.3547 - val_main_output_loss: 0.3424 - val_aux_output_loss: 0.4656\n"
     ]
    }
   ],
   "source": [
    "# We need to provide labels for each output:\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3702 - main_output_loss: 0.3599 - aux_output_loss: 0.4630\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3520784]\n",
      " [3.307941 ]\n",
      " [2.219055 ]] [[2.161956 ]\n",
      " [3.3371372]\n",
      " [2.0777373]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "print(y_pred_main, y_pred_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load a model\n",
    "model.save(\"my_model.h5\")\n",
    "load_model = model.load(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: save the model when something happen\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"my_model2.h5\")\n",
    "history2 = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint])\n",
    "# using save_best_only=True, saves the best perfomance in the validation set\n",
    "checkpoint2 = keras.callbacks.ModelCheckpoint(\"my_model3.h5\", save_best_only=True)\n",
    "history2 = model.fit(X_train, y_train, epochs=10,\n",
    "                     validation_data=(X_valid, y_valid),\n",
    "                     callbacks=[checkpoint2])\n",
    "# rolling back to the best model\n",
    "best_model = keras.models.load_model(\"my_model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping: stop when no progress is made in a number of epochs\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history2 = model.fit(X_train, y_train, epochs=100,\n",
    "                     validation_data=(X_valid, y_valid),\n",
    "                     callbacks=[checkpoint2, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom callback, print something after epoch\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "# There are a lot of implementations for a callback, page 316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Subclassing API to build dynamic models\n",
    "\n",
    "The __Sequential API__ and the __Functional API__ are really useful, the model can be saved, clone, shared, it can be analized, the framework can infere shapes and check type... but it is static. The __Subclassing API__ allows us to make models with loops, conditionals, varying shapes, etc, but it doesn't have the benefits of the first two APIs. We can save and restore manually some parameters with _save_weights()_ and _load_weights()_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args, e.g. name\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # this method is versatile\n",
    "        inputA, inputB = inputs\n",
    "        hidden1 = self.hidden1(inputB)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([inputA, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.4381 - output_1_loss: 2.2326 - output_2_loss: 4.2876 - val_loss: 1.1334 - val_output_1_loss: 0.8985 - val_output_2_loss: 3.2469\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0177 - output_1_loss: 0.8270 - output_2_loss: 2.7339 - val_loss: 0.8445 - val_output_1_loss: 0.6938 - val_output_2_loss: 2.2005\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.8293 - output_1_loss: 0.6949 - output_2_loss: 2.0382 - val_loss: 0.7408 - val_output_1_loss: 0.6254 - val_output_2_loss: 1.7793\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7547 - output_1_loss: 0.6428 - output_2_loss: 1.7620 - val_loss: 0.6867 - val_output_1_loss: 0.5834 - val_output_2_loss: 1.6161\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7111 - output_1_loss: 0.6075 - output_2_loss: 1.6427 - val_loss: 0.6538 - val_output_1_loss: 0.5555 - val_output_2_loss: 1.5385\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6831 - output_1_loss: 0.5842 - output_2_loss: 1.5732 - val_loss: 0.6288 - val_output_1_loss: 0.5331 - val_output_2_loss: 1.4900\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6601 - output_1_loss: 0.5644 - output_2_loss: 1.5217 - val_loss: 0.6099 - val_output_1_loss: 0.5167 - val_output_2_loss: 1.4492\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6408 - output_1_loss: 0.5482 - output_2_loss: 1.4747 - val_loss: 0.5914 - val_output_1_loss: 0.5004 - val_output_2_loss: 1.4110\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6232 - output_1_loss: 0.5332 - output_2_loss: 1.4334 - val_loss: 0.5777 - val_output_1_loss: 0.4891 - val_output_2_loss: 1.3748\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6089 - output_1_loss: 0.5217 - output_2_loss: 1.3941 - val_loss: 0.5664 - val_output_1_loss: 0.4803 - val_output_2_loss: 1.3412\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5965 - output_1_loss: 0.5117 - output_2_loss: 1.3591 - val_loss: 0.5554 - val_output_1_loss: 0.4712 - val_output_2_loss: 1.3123\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5871 - output_1_loss: 0.5048 - output_2_loss: 1.3285 - val_loss: 0.5465 - val_output_1_loss: 0.4643 - val_output_2_loss: 1.2859\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5789 - output_1_loss: 0.4987 - output_2_loss: 1.3013 - val_loss: 0.5382 - val_output_1_loss: 0.4578 - val_output_2_loss: 1.2618\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5705 - output_1_loss: 0.4920 - output_2_loss: 1.2763 - val_loss: 0.5316 - val_output_1_loss: 0.4530 - val_output_2_loss: 1.2388\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5639 - output_1_loss: 0.4874 - output_2_loss: 1.2525 - val_loss: 0.5260 - val_output_1_loss: 0.4492 - val_output_2_loss: 1.2176\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5601 - output_1_loss: 0.4855 - output_2_loss: 1.2318 - val_loss: 0.5199 - val_output_1_loss: 0.4445 - val_output_2_loss: 1.1981\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5525 - output_1_loss: 0.4794 - output_2_loss: 1.2107 - val_loss: 0.5154 - val_output_1_loss: 0.4416 - val_output_2_loss: 1.1803\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5468 - output_1_loss: 0.4752 - output_2_loss: 1.1916 - val_loss: 0.5096 - val_output_1_loss: 0.4372 - val_output_2_loss: 1.1613\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5426 - output_1_loss: 0.4726 - output_2_loss: 1.1728 - val_loss: 0.5053 - val_output_1_loss: 0.4342 - val_output_2_loss: 1.1449\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5378 - output_1_loss: 0.4691 - output_2_loss: 1.1559 - val_loss: 0.5023 - val_output_1_loss: 0.4328 - val_output_2_loss: 1.1279\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.5273 - output_1_loss: 0.4600 - output_2_loss: 1.1329\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd95d467b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B),(y_train, y_train),\n",
    "             epochs=20, validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning Neural network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    # Simple sequential model for univariate regression\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers(keras.optimizers.SGD(lr=learning_rate))\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV or RandomizedSearchCV from scikit-learn into Keras\n",
    "# it is like a regular Scikit-Learn regressor: fit(), score(), predict()\n",
    "keras_regressor = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "# Any extra parameter will be passed to the underlying Keras model\n",
    "keras_regressor.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_regressor.score(X_test, y_test)\n",
    "y_pred = keras_regressor.predict(X_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "# RandomizedSearchCV uses K-fold cross-validation, it doesn't use the validation\n",
    "# data, this is only used for early stopping\n",
    "rnd_search_cv = RandomizedSearchCV(keras_regressor, param_distributions, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accesing the best parameters\n",
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)\n",
    "# saving\n",
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tips on tuning\n",
    "\n",
    "Using the same number of neurons in all hidden layer perform just as well as a piramid (more neurons at the start).\n",
    "\n",
    "It is practical to use a model with more layers and neurons than needed, just use early stopping and other regularization techniques.\n",
    "\n",
    "__learning rate:__ Ideal would be to increase it per epoch by some little magnitude and when the loss explodes, pick a lr before it happened.\n",
    "\n",
    "__batch size:__ Small (2-32) is almost good anytime. If we have a GPU it can be larger (up to 8192 for e.g.), and using learning rate warm up, it lead to small training time.\n",
    "\n",
    "__update always the learning rate if other parameter is changed__"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
